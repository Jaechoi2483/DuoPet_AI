"""
ëˆˆ ì§ˆí™˜ ëª¨ë¸ Normalization ë¬¸ì œ í•´ê²°
ì§ì ‘ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí•˜ê³  ìƒˆ ëª¨ë¸ì— ì ìš©
"""
import os
import tensorflow as tf
import numpy as np
from pathlib import Path
import json
import h5py
import pickle

# TensorFlow 2.x ì„¤ì •
tf.config.run_functions_eagerly(True)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

class EyeModelNormalizationFixer:
    """Normalization ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë³€í™˜ê¸°"""
    
    def __init__(self):
        self.models_dir = Path("models/health_diagnosis/eye_disease")
        self.original_model_path = self.models_dir / "best_grouped_model.keras"
        self.fixed_model_path = self.models_dir / "eye_disease_fixed.h5"
        self.output_path = self.models_dir / "eye_disease_tf2_final.h5"
        
    def extract_weights_from_h5(self):
        """H5 íŒŒì¼ì—ì„œ ì§ì ‘ ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        print("ğŸ“¦ H5 íŒŒì¼ì—ì„œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì¤‘...")
        
        weights_dict = {}
        
        # ë¨¼ì € fixed ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if self.fixed_model_path.exists():
            h5_path = self.fixed_model_path
            print(f"  - ì‚¬ìš© íŒŒì¼: {h5_path.name}")
        else:
            h5_path = self.original_model_path
            print(f"  - ì‚¬ìš© íŒŒì¼: {h5_path.name}")
        
        try:
            with h5py.File(h5_path, 'r') as f:
                def extract_weights(name, obj):
                    if isinstance(obj, h5py.Dataset):
                        weights_dict[name] = np.array(obj)
                        print(f"    âœ“ {name}: shape={obj.shape}")
                
                # model_weights ê·¸ë£¹ì—ì„œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
                if 'model_weights' in f:
                    f['model_weights'].visititems(extract_weights)
                else:
                    # ë‹¤ë¥¸ êµ¬ì¡°ì¼ ê²½ìš°
                    f.visititems(extract_weights)
                    
            print(f"  âœ“ ì´ {len(weights_dict)}ê°œ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì™„ë£Œ")
            return weights_dict
            
        except Exception as e:
            print(f"  âŒ ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def create_clean_efficientnet_model(self):
        """Normalization ì—†ëŠ” ê¹¨ë—í•œ EfficientNet ëª¨ë¸ ìƒì„±"""
        print("\nğŸ”§ ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì¤‘...")
        
        # ì…ë ¥ ë ˆì´ì–´
        inputs = tf.keras.Input(shape=(224, 224, 3), name='input_1')
        
        # EfficientNet ì „ì²˜ë¦¬ (Normalization ëŒ€ì²´)
        x = tf.keras.layers.Rescaling(scale=1./255.0, name='rescaling')(inputs)
        x = tf.keras.layers.Lambda(
            lambda x: (x - 0.5) * 2.0,  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
            name='preprocessing'
        )(x)
        
        # EfficientNetB0 ë°±ë³¸
        base_model = tf.keras.applications.EfficientNetB0(
            input_tensor=x,
            include_top=False,
            weights='imagenet',  # ImageNet ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”
            input_shape=(224, 224, 3)
        )
        base_model.trainable = True
        
        # ë°±ë³¸ ì¶œë ¥
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)
        
        # Dense layers
        x = tf.keras.layers.Dense(128, activation='relu', name='dense')(x)
        x = tf.keras.layers.Dropout(0.5, name='dropout')(x)
        outputs = tf.keras.layers.Dense(5, activation='softmax', name='dense_1')(x)
        
        # ëª¨ë¸ ìƒì„±
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        
        print("  âœ“ ëª¨ë¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
        print(f"  - ì´ ë ˆì´ì–´: {len(model.layers)}")
        print(f"  - íŒŒë¼ë¯¸í„°: {model.count_params():,}")
        
        return model
    
    def apply_extracted_weights(self, model, weights_dict):
        """ì¶”ì¶œí•œ ê°€ì¤‘ì¹˜ë¥¼ ìƒˆ ëª¨ë¸ì— ì ìš©"""
        print("\nğŸ”„ ê°€ì¤‘ì¹˜ ì ìš© ì¤‘...")
        
        applied_count = 0
        skipped_count = 0
        
        for layer in model.layers:
            layer_name = layer.name
            
            # Dense ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë§¤í•‘
            if isinstance(layer, tf.keras.layers.Dense):
                # ê°€ëŠ¥í•œ í‚¤ íŒ¨í„´ë“¤
                kernel_keys = [
                    f"{layer_name}/{layer_name}/kernel:0",
                    f"{layer_name}/kernel:0",
                    f"dense/{layer_name}/kernel:0",
                    f"model/layers/{layer_name}/kernel:0"
                ]
                bias_keys = [
                    f"{layer_name}/{layer_name}/bias:0",
                    f"{layer_name}/bias:0",
                    f"dense/{layer_name}/bias:0",
                    f"model/layers/{layer_name}/bias:0"
                ]
                
                # ë§¤ì¹­ë˜ëŠ” ê°€ì¤‘ì¹˜ ì°¾ê¸°
                kernel_weight = None
                bias_weight = None
                
                for k_key in kernel_keys:
                    if k_key in weights_dict:
                        kernel_weight = weights_dict[k_key]
                        break
                
                for b_key in bias_keys:
                    if b_key in weights_dict:
                        bias_weight = weights_dict[b_key]
                        break
                
                if kernel_weight is not None and bias_weight is not None:
                    try:
                        # shape í™•ì¸
                        expected_kernel_shape = layer.kernel.shape
                        expected_bias_shape = layer.bias.shape
                        
                        if (kernel_weight.shape == expected_kernel_shape and 
                            bias_weight.shape == expected_bias_shape):
                            layer.set_weights([kernel_weight, bias_weight])
                            print(f"  âœ“ {layer_name} ê°€ì¤‘ì¹˜ ì ìš© ì™„ë£Œ")
                            applied_count += 1
                        else:
                            print(f"  âš ï¸ {layer_name} shape ë¶ˆì¼ì¹˜ - ìŠ¤í‚µ")
                            skipped_count += 1
                    except Exception as e:
                        print(f"  âš ï¸ {layer_name} ì ìš© ì‹¤íŒ¨: {e}")
                        skipped_count += 1
            
            # GlobalAveragePooling2DëŠ” ê°€ì¤‘ì¹˜ ì—†ìŒ
            elif isinstance(layer, tf.keras.layers.GlobalAveragePooling2D):
                continue
                
            # Dropoutì€ ê°€ì¤‘ì¹˜ ì—†ìŒ
            elif isinstance(layer, tf.keras.layers.Dropout):
                continue
        
        print(f"\nğŸ“Š ê°€ì¤‘ì¹˜ ì ìš© ê²°ê³¼:")
        print(f"  - ì ìš©ë¨: {applied_count}ê°œ")
        print(f"  - ìŠ¤í‚µë¨: {skipped_count}ê°œ")
        
        return applied_count > 0
    
    def convert_model(self):
        """ëª¨ë¸ ë³€í™˜ ì‹¤í–‰"""
        print("ğŸš€ ëˆˆ ì§ˆí™˜ ëª¨ë¸ ë³€í™˜ ì‹œì‘")
        print("=" * 60)
        
        try:
            # 1. ê°€ì¤‘ì¹˜ ì¶”ì¶œ
            weights_dict = self.extract_weights_from_h5()
            if not weights_dict:
                # ëŒ€ì•ˆ: ê¸°ì¡´ fixed ëª¨ë¸ ì‚¬ìš©
                if self.fixed_model_path.exists():
                    print("\nğŸ“Œ ê¸°ì¡´ fixed ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    model = tf.keras.models.load_model(
                        str(self.fixed_model_path),
                        custom_objects={'swish': tf.nn.swish},
                        compile=False
                    )
                else:
                    raise ValueError("ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨ ë° ëŒ€ì²´ ëª¨ë¸ ì—†ìŒ")
            else:
                # 2. ìƒˆ ëª¨ë¸ ìƒì„±
                model = self.create_clean_efficientnet_model()
                
                # 3. ê°€ì¤‘ì¹˜ ì ìš©
                success = self.apply_extracted_weights(model, weights_dict)
                if not success:
                    print("âš ï¸ ê°€ì¤‘ì¹˜ ì ìš© ì‹¤íŒ¨ - ImageNet ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            
            # 4. ëª¨ë¸ ì»´íŒŒì¼
            print("\nğŸ”¨ ëª¨ë¸ ì»´íŒŒì¼ ì¤‘...")
            model.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                loss='categorical_crossentropy',
                metrics=['accuracy', 
                        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')]
            )
            
            # 5. ì €ì¥
            print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
            model.save(str(self.output_path), save_format='h5')
            print(f"  âœ“ ì €ì¥ ì™„ë£Œ: {self.output_path}")
            
            # 6. ê²€ì¦
            print("\nâœ… ë³€í™˜ ê²€ì¦ ì¤‘...")
            test_input = np.random.randn(1, 224, 224, 3).astype(np.float32) * 255
            output = model.predict(test_input, verbose=0)
            
            print(f"  - ì…ë ¥ shape: {test_input.shape}")
            print(f"  - ì¶œë ¥ shape: {output.shape}")
            print(f"  - ì¶œë ¥ ê°’: {output[0]}")
            print(f"  - í•©ê³„: {np.sum(output[0]):.4f}")
            
            # ëª¨ë¸ ì •ë³´ ì €ì¥
            model_info = {
                "model_name": "eye_disease_model",
                "input_shape": [224, 224, 3],
                "output_classes": 5,
                "class_names": ["ì •ìƒ", "ë°±ë‚´ì¥", "ë…¹ë‚´ì¥", "ë§ë§‰ì§ˆí™˜", "ê°ë§‰ì§ˆí™˜"],
                "preprocessing": "rescaling_and_normalization",
                "framework": "tensorflow",
                "version": tf.__version__,
                "conversion_notes": "Normalization layer replaced with Rescaling + Lambda"
            }
            
            info_path = self.models_dir / "model_info_tf2.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            print(f"\nâœ… ë³€í™˜ ì™„ë£Œ!")
            print(f"  - ëª¨ë¸: {self.output_path}")
            print(f"  - ì •ë³´: {info_path}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def test_converted_model(self):
        """ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        if not self.output_path.exists():
            print("âŒ ë³€í™˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ëª¨ë¸ ë¡œë“œ
            model = tf.keras.models.load_model(str(self.output_path))
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            test_images = np.random.randint(0, 255, (3, 224, 224, 3), dtype=np.uint8)
            test_images = test_images.astype(np.float32)
            
            # ì˜ˆì¸¡
            predictions = model.predict(test_images, verbose=0)
            
            # ê²°ê³¼ ì¶œë ¥
            print("\nì˜ˆì¸¡ ê²°ê³¼:")
            class_names = ["ì •ìƒ", "ë°±ë‚´ì¥", "ë…¹ë‚´ì¥", "ë§ë§‰ì§ˆí™˜", "ê°ë§‰ì§ˆí™˜"]
            
            for i, pred in enumerate(predictions):
                predicted_class = np.argmax(pred)
                confidence = pred[predicted_class]
                
                print(f"\nì´ë¯¸ì§€ {i+1}:")
                print(f"  - ì˜ˆì¸¡: {class_names[predicted_class]}")
                print(f"  - ì‹ ë¢°ë„: {confidence:.2%}")
                print(f"  - ìƒìœ„ 2ê°œ:")
                top_2 = np.argsort(pred)[-2:][::-1]
                for idx in top_2:
                    print(f"    â€¢ {class_names[idx]}: {pred[idx]:.2%}")
            
            print("\nâœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    fixer = EyeModelNormalizationFixer()
    
    # ë³€í™˜ ì‹¤í–‰
    if fixer.convert_model():
        # í…ŒìŠ¤íŠ¸
        fixer.test_converted_model()