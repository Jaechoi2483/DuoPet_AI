# ğŸ¾ DuoPet AI ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ëª¨ë¸ í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì‚¬ì „ ì¤€ë¹„ì‚¬í•­](#ì‚¬ì „-ì¤€ë¹„ì‚¬í•­)
3. [Step 1: ë°ì´í„° ì¤€ë¹„ ë° ì—…ë¡œë“œ](#step-1-ë°ì´í„°-ì¤€ë¹„-ë°-ì—…ë¡œë“œ)
4. [Step 2: Google Colab í™˜ê²½ ì„¤ì •](#step-2-google-colab-í™˜ê²½-ì„¤ì •)
5. [Step 3: ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬](#step-3-ë°ì´í„°-ì¤€ë¹„-ë°-ì „ì²˜ë¦¬)
6. [Step 4: ëª¨ë¸ í•™ìŠµ ì½”ë“œ ì‘ì„±](#step-4-ëª¨ë¸-í•™ìŠµ-ì½”ë“œ-ì‘ì„±)
7. [Step 5: ëª¨ë¸ í•™ìŠµ ì‹¤í–‰](#step-5-ëª¨ë¸-í•™ìŠµ-ì‹¤í–‰)
8. [Step 6: ëª¨ë¸ í‰ê°€ ë° ì €ì¥](#step-6-ëª¨ë¸-í‰ê°€-ë°-ì €ì¥)
9. [Step 7: DuoPet AI í”„ë¡œì íŠ¸ì— í†µí•©](#step-7-duopet-ai-í”„ë¡œì íŠ¸ì—-í†µí•©)
10. [ë¬¸ì œ í•´ê²° ê°€ì´ë“œ](#ë¬¸ì œ-í•´ê²°-ê°€ì´ë“œ)

---

## ê°œìš”

ì´ ê°€ì´ë“œëŠ” **E:\DATA\153.ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ë°ì´í„°**ë¥¼ ì‚¬ìš©í•˜ì—¬ Google Colabì—ì„œ ì•ˆêµ¬ì§ˆí™˜ ì§„ë‹¨ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤. 

### ğŸ¯ ëª©í‘œ
- TensorFlow 2.14.0ì„ ì‚¬ìš©í•œ ì•ˆêµ¬ì§ˆí™˜ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ
- ê¸°ì¡´ í”¼ë¶€ì§ˆí™˜ ëª¨ë¸ê³¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
- DuoPet AI ì„œë¹„ìŠ¤ì— ì¦‰ì‹œ í†µí•© ê°€ëŠ¥í•œ ëª¨ë¸ ìƒì„±

### ğŸ“Š ëª¨ë¸ ì‚¬ì–‘
- **ì…ë ¥ í¬ê¸°**: 224x224x3 (RGB ì´ë¯¸ì§€)
- **ì •ê·œí™”**: ImageNet í‘œì¤€ ì •ê·œí™”
- **ëª¨ë¸ í˜•ì‹**: TensorFlow SavedModel ë˜ëŠ” Keras .h5
- **ì˜ˆìƒ ì§ˆí™˜ ë¶„ë¥˜**: ì •ìƒ, ë°±ë‚´ì¥, ê²°ë§‰ì—¼, ê°ë§‰ê¶¤ì–‘ ë“±

---

## ì‚¬ì „ ì¤€ë¹„ì‚¬í•­

### í•„ìš”í•œ ê²ƒë“¤:
- âœ… Google ê³„ì •
- âœ… Google Drive ì €ì¥ ê³µê°„ (ìµœì†Œ 10GB)
- âœ… E:\DATA\153.ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ë°ì´í„°
- âœ… ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²°

---

## Step 1: ë°ì´í„° ì¤€ë¹„ ë° ì—…ë¡œë“œ

### 1.1 ë°ì´í„° ì••ì¶•
```powershell
# PowerShell ë˜ëŠ” Windows íƒìƒ‰ê¸°ì—ì„œ
# E:\DATA\153.ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ë°ì´í„° í´ë”ë¥¼ ìš°í´ë¦­
# "ì••ì¶•(ZIP) í´ë”ë¡œ ë³´ë‚´ê¸°" ì„ íƒ
# íŒŒì¼ëª…: eye_disease_dataset.zip
```

### 1.2 Google Drive í´ë” êµ¬ì¡° ìƒì„±
```
Google Drive/
â””â”€â”€ DuoPet_AI_Training/
    â”œâ”€â”€ datasets/
    â”‚   â””â”€â”€ eye_disease_dataset.zip
    â”œâ”€â”€ models/
    â””â”€â”€ logs/
```

### 1.3 íŒŒì¼ ì—…ë¡œë“œ
1. [Google Drive](https://drive.google.com) ì ‘ì†
2. `DuoPet_AI_Training/datasets/` í´ë”ì— `eye_disease_dataset.zip` ì—…ë¡œë“œ
3. ì—…ë¡œë“œ ì™„ë£Œ í™•ì¸ (íŒŒì¼ í¬ê¸°ì™€ ëŒ€ì¡°)

---

## Step 2: Google Colab í™˜ê²½ ì„¤ì •

### 2.1 ìƒˆ ë…¸íŠ¸ë¶ ìƒì„±
1. [Google Colab](https://colab.research.google.com) ì ‘ì†
2. "ìƒˆ ë…¸íŠ¸ë¶" í´ë¦­
3. ë…¸íŠ¸ë¶ ì´ë¦„ì„ `DuoPet_Eye_Disease_Training.ipynb`ë¡œ ë³€ê²½

### 2.2 GPU ëŸ°íƒ€ì„ ì„¤ì •
```python
# ì²« ë²ˆì§¸ ì…€: GPU ì„¤ì • í™•ì¸
# ë©”ë‰´: ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸°: GPU (T4 ì¶”ì²œ)

!nvidia-smi
```

### 2.3 Google Drive ë§ˆìš´íŠ¸
```python
from google.colab import drive
drive.mount('/content/drive')

# ë§ˆìš´íŠ¸ í™•ì¸
!ls /content/drive/MyDrive/DuoPet_AI_Training/datasets/
```

---

## Step 3: ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬

### 3.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```python
# DuoPet AIì™€ ë™ì¼í•œ í™˜ê²½ êµ¬ì„±
!pip install tensorflow==2.14.0
!pip install opencv-python==4.8.1.78
!pip install pandas numpy matplotlib seaborn
!pip install scikit-learn
!pip install albumentations

# ì„¤ì¹˜ í™•ì¸
import tensorflow as tf
print(f"TensorFlow version: {tf.__version__}")
print(f"GPU available: {tf.config.list_physical_devices('GPU')}")
```

### 3.2 ë°ì´í„° ì••ì¶• í•´ì œ
```python
import zipfile
import os

# ì••ì¶• í•´ì œ
zip_path = "/content/drive/MyDrive/DuoPet_AI_Training/datasets/eye_disease_dataset.zip"
extract_path = "/content/eye_disease_data"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

# ë°ì´í„° êµ¬ì¡° í™•ì¸
!find /content/eye_disease_data -type d -name "*" | head -20
```

### 3.3 ë°ì´í„° íƒìƒ‰ ë° ì´í•´
```python
import os
import pandas as pd
from pathlib import Path

# ë°ì´í„° êµ¬ì¡° íŒŒì•…
data_root = Path("/content/eye_disease_data")

# í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìˆ˜ í™•ì¸
class_counts = {}
for class_dir in data_root.iterdir():
    if class_dir.is_dir():
        image_count = len(list(class_dir.glob("*.jpg")) + list(class_dir.glob("*.png")))
        class_counts[class_dir.name] = image_count

print("í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìˆ˜:")
for class_name, count in sorted(class_counts.items()):
    print(f"  {class_name}: {count}ê°œ")
```

---

## Step 4: ëª¨ë¸ í•™ìŠµ ì½”ë“œ ì‘ì„±

### 4.1 ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
```python
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

# DuoPet AIì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì„¤ì •
IMG_SIZE = 224
BATCH_SIZE = 32

# ImageNet ì •ê·œí™” ê°’
IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])
IMAGENET_STD = np.array([0.229, 0.224, 0.225])

def preprocess_input(x):
    """DuoPet AIì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©"""
    x = x / 255.0
    x = (x - IMAGENET_MEAN) / IMAGENET_STD
    return x

# ë°ì´í„° ì¦ê°• ì„¤ì • (í•™ìŠµìš©)
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.15,
    validation_split=0.2
)

# ê²€ì¦/í…ŒìŠ¤íŠ¸ìš© (ì¦ê°• ì—†ìŒ)
val_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    validation_split=0.2
)
```

### 4.2 ë°ì´í„° ë¡œë” ìƒì„±
```python
# í•™ìŠµ ë°ì´í„° ë¡œë”
train_generator = train_datagen.flow_from_directory(
    data_root,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

# ê²€ì¦ ë°ì´í„° ë¡œë”
validation_generator = val_datagen.flow_from_directory(
    data_root,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# í´ë˜ìŠ¤ ì •ë³´ ì €ì¥
class_indices = train_generator.class_indices
num_classes = len(class_indices)
print(f"ì´ í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
print(f"í´ë˜ìŠ¤ ë§¤í•‘: {class_indices}")

# í´ë˜ìŠ¤ ì´ë¦„ ì €ì¥ (ë‚˜ì¤‘ì— ì‚¬ìš©)
import json
class_names = {v: k for k, v in class_indices.items()}
with open('/content/class_names.json', 'w', encoding='utf-8') as f:
    json.dump(class_names, f, ensure_ascii=False, indent=2)
```

### 4.3 ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
```python
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0

def create_eye_disease_model(num_classes):
    """
    DuoPet AIì˜ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ì¼ê´€ëœ ì•„í‚¤í…ì²˜ ì‚¬ìš©
    EfficientNetB0 ê¸°ë°˜ ì „ì´í•™ìŠµ ëª¨ë¸
    """
    # ì‚¬ì „í•™ìŠµëœ EfficientNetB0 (ImageNet weights)
    base_model = EfficientNetB0(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet'
    )
    
    # Fine-tuningì„ ìœ„í•´ ìƒìœ„ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
    base_model.trainable = True
    fine_tune_at = 100
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    
    # ì»¤ìŠ¤í…€ ë¶„ë¥˜ í—¤ë“œ
    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = base_model(inputs, training=False)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    
    model = models.Model(inputs, outputs)
    
    return model

# ëª¨ë¸ ìƒì„±
model = create_eye_disease_model(num_classes)
model.summary()
```

### 4.4 í•™ìŠµ ì„¤ì •
```python
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import os

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
optimizer = Adam(learning_rate=0.0001)

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

# ì½œë°± ì„¤ì •
checkpoint_path = "/content/drive/MyDrive/DuoPet_AI_Training/models/eye_disease_best_model.h5"
callbacks = [
    ModelCheckpoint(
        checkpoint_path,
        monitor='val_accuracy',
        save_best_only=True,
        save_weights_only=False,
        mode='max',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7,
        verbose=1
    )
]
```

---

## Step 5: ëª¨ë¸ í•™ìŠµ ì‹¤í–‰

### 5.1 í•™ìŠµ ì‹œì‘
```python
# ì—í¬í¬ ìˆ˜ ì„¤ì •
EPOCHS = 50

# í•™ìŠµ ì‹¤í–‰
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=validation_generator,
    callbacks=callbacks,
    verbose=1
)

print("í•™ìŠµ ì™„ë£Œ!")
```

### 5.2 í•™ìŠµ ê³¼ì • ì‹œê°í™”
```python
import matplotlib.pyplot as plt

def plot_training_history(history):
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # ì •í™•ë„ ê·¸ë˜í”„
    axes[0].plot(history.history['accuracy'], label='Training Accuracy')
    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
    axes[0].set_title('Model Accuracy')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Accuracy')
    axes[0].legend()
    axes[0].grid(True)
    
    # ì†ì‹¤ ê·¸ë˜í”„
    axes[1].plot(history.history['loss'], label='Training Loss')
    axes[1].plot(history.history['val_loss'], label='Validation Loss')
    axes[1].set_title('Model Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True)
    
    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/DuoPet_AI_Training/training_history.png')
    plt.show()

plot_training_history(history)
```

---

## Step 6: ëª¨ë¸ í‰ê°€ ë° ì €ì¥

### 6.1 ëª¨ë¸ í‰ê°€
```python
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# ìµœì¢… ëª¨ë¸ ë¡œë“œ
best_model = tf.keras.models.load_model(checkpoint_path)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€
test_generator = val_datagen.flow_from_directory(
    data_root,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=1,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# ì˜ˆì¸¡ ìˆ˜í–‰
predictions = best_model.predict(test_generator, verbose=1)
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\n=== ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ===")
print(classification_report(y_true, y_pred, 
                          target_names=list(class_names.values())))

# í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=list(class_names.values()),
            yticklabels=list(class_names.values()))
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/DuoPet_AI_Training/confusion_matrix.png')
plt.show()
```

### 6.2 ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
```python
# 1. Keras H5 í˜•ì‹ (ì´ë¯¸ ì €ì¥ë¨)
print(f"H5 ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {checkpoint_path}")

# 2. TensorFlow SavedModel í˜•ì‹ (ê¶Œì¥)
savedmodel_path = "/content/drive/MyDrive/DuoPet_AI_Training/models/eye_disease_savedmodel"
best_model.save(savedmodel_path)
print(f"SavedModel ì €ì¥ ìœ„ì¹˜: {savedmodel_path}")

# 3. ëª¨ë¸ ì„¤ì • íŒŒì¼ ìƒì„±
config = {
    "model_type": "eye_disease_classification",
    "architecture": "EfficientNetB0",
    "input_shape": [224, 224, 3],
    "num_classes": num_classes,
    "class_names": class_names,
    "preprocessing": {
        "normalization": "imagenet",
        "mean": IMAGENET_MEAN.tolist(),
        "std": IMAGENET_STD.tolist()
    },
    "training_info": {
        "dataset": "153.ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ë°ì´í„°",
        "epochs": EPOCHS,
        "best_val_accuracy": float(max(history.history['val_accuracy'])),
        "final_val_loss": float(history.history['val_loss'][-1])
    }
}

config_path = "/content/drive/MyDrive/DuoPet_AI_Training/models/eye_disease_config.json"
with open(config_path, 'w', encoding='utf-8') as f:
    json.dump(config, f, ensure_ascii=False, indent=2)
print(f"ì„¤ì • íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {config_path}")
```

### 6.3 ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
```python
def test_model_inference(model_path, test_image_path):
    """ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    import cv2
    
    # ëª¨ë¸ ë¡œë“œ
    model = tf.keras.models.load_model(model_path)
    
    # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
    img = cv2.imread(test_image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = preprocess_input(img)
    img = np.expand_dims(img, axis=0)
    
    # ì˜ˆì¸¡
    predictions = model.predict(img)
    predicted_class = np.argmax(predictions[0])
    confidence = predictions[0][predicted_class]
    
    print(f"ì˜ˆì¸¡ í´ë˜ìŠ¤: {class_names[predicted_class]}")
    print(f"ì‹ ë¢°ë„: {confidence:.2%}")
    
    return predicted_class, confidence

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜ˆì‹œ)
# test_model_inference(checkpoint_path, "/path/to/test/image.jpg")
```

---

## Step 7: DuoPet AI í”„ë¡œì íŠ¸ì— í†µí•©

### 7.1 ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
1. Google Driveì—ì„œ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œ:
   - `eye_disease_best_model.h5`
   - `eye_disease_savedmodel/` (í´ë” ì „ì²´)
   - `eye_disease_config.json`
   - `class_names.json`

2. ë¡œì»¬ ì €ì¥ ìœ„ì¹˜:
   ```
   D:\final_project\DuoPet_AI\models\health_diagnosis\eye_disease\
   â”œâ”€â”€ eye_disease_model.h5
   â”œâ”€â”€ eye_disease_savedmodel/
   â”œâ”€â”€ config.json
   â””â”€â”€ class_names.json
   ```

### 7.2 DuoPet AI ì½”ë“œ ìˆ˜ì •
```python
# services/health_diagnosis/predict.pyì— ì¶”ê°€í•  ì½”ë“œ

class EyeDiseasePredictor:
    def __init__(self, model_path):
        self.model = tf.keras.models.load_model(model_path)
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë¡œë“œ
        with open(os.path.join(os.path.dirname(model_path), 'class_names.json'), 'r', encoding='utf-8') as f:
            self.class_names = json.load(f)
        
        # ì„¤ì • ë¡œë“œ
        with open(os.path.join(os.path.dirname(model_path), 'config.json'), 'r', encoding='utf-8') as f:
            self.config = json.load(f)
    
    def predict(self, image_path):
        """ì•ˆêµ¬ì§ˆí™˜ ì˜ˆì¸¡"""
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        img = cv2.imread(image_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (224, 224))
        img = self._preprocess_input(img)
        img = np.expand_dims(img, axis=0)
        
        # ì˜ˆì¸¡
        predictions = self.model.predict(img)
        predicted_class = np.argmax(predictions[0])
        confidence = float(predictions[0][predicted_class])
        
        return {
            "disease_type": self.class_names[str(predicted_class)],
            "confidence": confidence,
            "all_predictions": {
                self.class_names[str(i)]: float(predictions[0][i]) 
                for i in range(len(predictions[0]))
            }
        }
    
    def _preprocess_input(self, x):
        """ImageNet ì •ê·œí™”"""
        x = x / 255.0
        x = (x - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
        return x
```

---

## ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ğŸ”§ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ë°©ë²•

#### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
BATCH_SIZE = 16  # 32ì—ì„œ 16ìœ¼ë¡œ ê°ì†Œ

# ë˜ëŠ” mixed precision ì‚¬ìš©
from tensorflow.keras.mixed_precision import Policy
policy = Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)
```

#### 2. í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ
```python
# Learning rate ì¡°ì •
optimizer = Adam(learning_rate=0.00001)  # ë” ì‘ì€ ê°’ìœ¼ë¡œ

# ë˜ëŠ” ë” ë§ì€ ë°ì´í„° ì¦ê°•
train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=30,  # ì¦ê°€
    width_shift_range=0.3,  # ì¦ê°€
    height_shift_range=0.3,  # ì¦ê°€
    horizontal_flip=True,
    vertical_flip=True,  # ì¶”ê°€
    zoom_range=0.2,
    brightness_range=[0.8, 1.2],  # ì¶”ê°€
    validation_split=0.2
)
```

#### 3. ê³¼ì í•© ë¬¸ì œ
```python
# Dropout ì¦ê°€
x = layers.Dropout(0.6)(x)  # 0.5ì—ì„œ 0.6ìœ¼ë¡œ

# L2 ì •ê·œí™” ì¶”ê°€
x = layers.Dense(256, activation='relu', 
                 kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)
```

#### 4. í´ë˜ìŠ¤ ë¶ˆê· í˜•
```python
# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weight_dict = dict(enumerate(class_weights))

# í•™ìŠµ ì‹œ ì ìš©
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=validation_generator,
    class_weight=class_weight_dict,  # ì¶”ê°€
    callbacks=callbacks
)
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— í™•ì¸í•˜ì„¸ìš”:

- [ ] Google Colab GPU ëŸ°íƒ€ì„ ì„¤ì • ì™„ë£Œ
- [ ] Google Drive ë§ˆìš´íŠ¸ ì„±ê³µ
- [ ] ë°ì´í„°ì…‹ ì••ì¶• íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ
- [ ] TensorFlow 2.14.0 ì„¤ì¹˜ í™•ì¸
- [ ] ë°ì´í„° êµ¬ì¡° ë° í´ë˜ìŠ¤ í™•ì¸
- [ ] ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ ì™„ë£Œ
- [ ] í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • (epochs, batch_size ë“±)
- [ ] ì €ì¥ ê²½ë¡œ ì„¤ì • ì™„ë£Œ

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

ì´ì œ DuoPet AI ì•ˆêµ¬ì§ˆí™˜ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. í•™ìŠµëœ ëª¨ë¸ì€ ê¸°ì¡´ í”¼ë¶€ì§ˆí™˜ ëª¨ë¸ê³¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ DuoPet AI ì„œë¹„ìŠ¤ì— ì¦‰ì‹œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‹¤ìŒ ë‹¨ê³„:
1. ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
2. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
3. í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™
4. ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬

ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë¬¸ì˜í•´ ì£¼ì„¸ìš”! ğŸ¶ğŸ±