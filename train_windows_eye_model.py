#!/usr/bin/env python3
"""
DuoPet AI ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Windows ë²„ì „)
- 14ê°œì˜ ì„¸ë¶„í™”ëœ ì§ˆë³‘ì„ 5ê°œì˜ ìƒìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë¶„ë¥˜
- Windows CPU í™˜ê²½ì— ìµœì í™”
- TensorFlow 2.x í˜¸í™˜, .h5 í˜•ì‹ ì €ì¥
"""

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import unicodedata
import json
import random
import platform
from pathlib import Path
from datetime import datetime
import time

# ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨
import numpy as np
import pandas as pd
from tqdm import tqdm

# TensorFlow/Keras import
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, Lambda
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from tensorflow.keras import backend as K

# scikit-learn
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

# ì‹œê°í™” ê´€ë ¨
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
matplotlib.use('Agg')  # Windows GUI ì—†ì´ ì‹¤í–‰

# í”Œë«í¼ë³„ í•œê¸€ í°íŠ¸ ì„¤ì •
def set_korean_font():
    system = platform.system()
    if system == 'Windows':
        plt.rcParams['font.family'] = 'Malgun Gothic'
    else:
        plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    print(f"âœ… í°íŠ¸ ì„¤ì • ì™„ë£Œ: {plt.rcParams['font.family']}")

set_korean_font()

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path("/mnt/d/final_project/DuoPet_AI")
DATASET_PATH = Path("E:/DATA/153.ë°˜ë ¤ë™ë¬¼ ì•ˆêµ¬ì§ˆí™˜ ë°ì´í„°/eye_disease_dataset")
TRAIN_DIR = DATASET_PATH / "Training"
VAL_DIR = DATASET_PATH / "Validation"
MODEL_DIR = BASE_DIR / "models" / "health_diagnosis" / "eye_disease"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# ì‹œë“œ ì„¤ì •
seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
IMG_SIZE = 224
BATCH_SIZE = 32  # Windows CPUì—ì„œë„ 32 ê°€ëŠ¥
EPOCHS = 30  # ì¶©ë¶„í•œ í•™ìŠµì„ ìœ„í•´ 30ìœ¼ë¡œ ì¦ê°€
LEARNING_RATE = 5e-5

# ì§ˆë³‘ ê·¸ë£¹í™” ë§¤í•‘ ì‚¬ì „
DISEASE_GROUP_MAP = {
    # ìˆ˜ì •ì²´ ì§ˆí™˜
    'ë°±ë‚´ì¥': 'ìˆ˜ì •ì²´ ì§ˆí™˜',
    'í•µê²½í™”': 'ìˆ˜ì •ì²´ ì§ˆí™˜',
    # ê°ë§‰ ì§ˆí™˜
    'ê°ë§‰ê¶¤ì–‘': 'ê°ë§‰ ì§ˆí™˜',
    'ê°ë§‰ë¶€ê³¨í¸': 'ê°ë§‰ ì§ˆí™˜',
    'ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜': 'ê°ë§‰ ì§ˆí™˜',
    'ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜': 'ê°ë§‰ ì§ˆí™˜',
    'ìƒ‰ì†Œì¹¨ì°©ì„±ê°ë§‰ì—¼': 'ê°ë§‰ ì§ˆí™˜',
    'ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì—¼': 'ê°ë§‰ ì§ˆí™˜',
    # ì•ˆê²€(ëˆˆêº¼í’€) ì§ˆí™˜
    'ì•ˆê²€ì—¼': 'ì•ˆê²€ ì§ˆí™˜',
    'ì•ˆê²€ì¢…ì–‘': 'ì•ˆê²€ ì§ˆí™˜',
    'ì•ˆê²€ë‚´ë°˜ì¦': 'ì•ˆê²€ ì§ˆí™˜',
    # ê²°ë§‰/ëˆ„ê´€ ì§ˆí™˜
    'ê²°ë§‰ì—¼': 'ê²°ë§‰ ë° ëˆ„ê´€ ì§ˆí™˜',
    'ìœ ë£¨ì¦': 'ê²°ë§‰ ë° ëˆ„ê´€ ì§ˆí™˜',
    # ì•ˆêµ¬ ë‚´ë¶€ ì§ˆí™˜
    'ìœ ë¦¬ì²´ë³€ì„±': 'ì•ˆêµ¬ ë‚´ë¶€ ì§ˆí™˜'
}

VALID_DISEASES = list(DISEASE_GROUP_MAP.keys())

# ëŒ€ë¶„ë¥˜ í´ë˜ìŠ¤ ë§¤í•‘
CLASS_MAP = {
    "ê°ë§‰ ì§ˆí™˜": 0,
    "ê²°ë§‰ ë° ëˆ„ê´€ ì§ˆí™˜": 1,
    "ìˆ˜ì •ì²´ ì§ˆí™˜": 2,
    "ì•ˆê²€ ì§ˆí™˜": 3,
    "ì•ˆêµ¬ ë‚´ë¶€ ì§ˆí™˜": 4
}

class DataProcessor:
    """ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, train_dir, val_dir):
        self.train_dir = train_dir
        self.val_dir = val_dir

    def collect_image_paths_and_labels(self, root_dir):
        """ì¬ê·€ì ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì¼ê³¼ ë ˆì´ë¸” ìˆ˜ì§‘"""
        image_data = []
        print(f"ğŸ“ {root_dir} ë””ë ‰í† ë¦¬ ê²€ìƒ‰ ì¤‘...")
        
        # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  jpg íŒŒì¼ ì°¾ê¸°
        image_files = list(Path(root_dir).rglob("*.jpg"))
        print(f"  ì´ {len(image_files)}ê°œì˜ JPG íŒŒì¼ ë°œê²¬")
        
        # ì§„í–‰ í‘œì‹œë¥¼ ìœ„í•œ tqdm ì‚¬ìš©
        for image_path in tqdm(image_files, desc=f"  {root_dir.name} ë°ì´í„° ì²˜ë¦¬ ì¤‘"):
            # ëŒ€ì‘í•˜ëŠ” JSON íŒŒì¼ ê²½ë¡œ
            json_path = image_path.with_suffix('.json')
            
            # ê²½ë¡œì—ì„œ ì§ˆë³‘ ì´ë¦„ ì¶”ì¶œ (ì—­ìˆœìœ¼ë¡œ ê²€ìƒ‰)
            disease_name = None
            for part in reversed(image_path.parts):
                normalized_part = unicodedata.normalize('NFC', part)
                if normalized_part in VALID_DISEASES:
                    disease_name = normalized_part
                    break
            
            if json_path.exists() and disease_name:
                # ì§ˆë³‘ ì´ë¦„ì„ ëŒ€ë¶„ë¥˜ë¡œ ë§¤í•‘
                grouped_label = DISEASE_GROUP_MAP.get(disease_name)
                if grouped_label:
                    image_data.append({
                        'path': str(image_path),
                        'label': grouped_label,
                        'original_disease': disease_name
                    })
        
        print(f"  ìœ íš¨í•œ ë°ì´í„° {len(image_data)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return pd.DataFrame(image_data)

    def prepare_data(self):
        """í•™ìŠµ ë° ê²€ì¦ ë°ì´í„° ì¤€ë¹„"""
        train_df = self.collect_image_paths_and_labels(self.train_dir)
        val_df = self.collect_image_paths_and_labels(self.val_dir)
        
        return train_df, val_df

class ModelPipeline:
    """ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    def __init__(self, train_df, val_df, class_map, epochs):
        self.train_df = train_df
        self.val_df = val_df
        self.class_map = class_map
        self.num_classes = len(class_map)
        self.epochs = epochs

    def preprocess_input_custom(self, image):
        """ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ í•¨ìˆ˜ - Lambda layerë¡œ êµ¬í˜„"""
        # EfficientNet ì „ì²˜ë¦¬ë¥¼ ì§ì ‘ êµ¬í˜„
        return image / 255.0

    def get_data_generators(self):
        """ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„±"""
        # ë°ì´í„° ì¦ê°• ì„¤ì •
        train_datagen = ImageDataGenerator(
            preprocessing_function=self.preprocess_input_custom,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            horizontal_flip=True,
            zoom_range=0.2,
            brightness_range=[0.8, 1.2],
            fill_mode='nearest'
        )
        
        val_datagen = ImageDataGenerator(
            preprocessing_function=self.preprocess_input_custom
        )

        # í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ì—­ë§¤í•‘
        class_indices = {v: k for k, v in self.class_map.items()}
        
        # ì œë„ˆë ˆì´í„° ìƒì„±
        train_generator = train_datagen.flow_from_dataframe(
            dataframe=self.train_df,
            x_col='path',
            y_col='label',
            target_size=(IMG_SIZE, IMG_SIZE),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            classes=list(self.class_map.keys()),
            shuffle=True
        )
        
        val_generator = val_datagen.flow_from_dataframe(
            dataframe=self.val_df,
            x_col='path',
            y_col='label',
            target_size=(IMG_SIZE, IMG_SIZE),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            classes=list(self.class_map.keys()),
            shuffle=False
        )
        
        return train_generator, val_generator

    def build_network(self):
        """EfficientNetB0 ê¸°ë°˜ ëª¨ë¸ êµ¬ì¶•"""
        print("\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¶• ì¤‘...")
        
        # ì…ë ¥ ë ˆì´ì–´
        input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, 3))
        
        # EfficientNetB0 ë°±ë³¸
        base_model = EfficientNetB0(
            include_top=False,
            weights='imagenet',
            input_tensor=input_tensor
        )
        
        # Fine-tuning ì„¤ì •: ë§ˆì§€ë§‰ 40ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
        base_model.trainable = True
        for layer in base_model.layers[:-40]:
            layer.trainable = False
        
        # ë¶„ë¥˜ í—¤ë“œ
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dropout(0.5)(x)
        x = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)
        x = Dropout(0.5)(x)
        output = Dense(self.num_classes, activation='softmax', name='disease_output')(x)
        
        # ëª¨ë¸ ìƒì„±
        model = Model(inputs=input_tensor, outputs=output)
        model.summary()
        
        return model

    def train(self):
        """ëª¨ë¸ í•™ìŠµ"""
        train_gen, val_gen = self.get_data_generators()
        model = self.build_network()
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer = Adam(learning_rate=LEARNING_RATE)
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_folder = MODEL_DIR / f"eye_disease_windows_{timestamp}"
        save_folder.mkdir(exist_ok=True)
        
        # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        best_model_path = str(save_folder / 'best_model.h5')
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            ModelCheckpoint(
                best_model_path,
                verbose=1,
                monitor='val_accuracy',
                save_best_only=True,
                mode='max'
            ),
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=3,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # ëª¨ë¸ ì»´íŒŒì¼
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy', tf.keras.metrics.Precision(name='precision'),
                     tf.keras.metrics.Recall(name='recall'),
                     tf.keras.metrics.AUC(name='auc')]
        )
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        class_weights = compute_class_weight(
            'balanced',
            classes=np.unique(train_gen.classes),
            y=train_gen.classes
        )
        class_weight_dict = dict(enumerate(class_weights))
        print("\nâš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©:")
        for idx, weight in class_weight_dict.items():
            class_name = list(self.class_map.keys())[idx]
            print(f"  {class_name}: {weight:.2f}")
        
        # í•™ìŠµ ì‹œì‘
        print("\nğŸš€ í•™ìŠµ ì‹œì‘...")
        history = model.fit(
            train_gen,
            validation_data=val_gen,
            epochs=self.epochs,
            callbacks=callbacks,
            class_weight=class_weight_dict,
            verbose=1
        )
        
        # ê²°ê³¼ ì €ì¥
        self.save_training_results(history, save_folder)
        
        return load_model(best_model_path), save_folder

    def save_training_results(self, history, save_folder):
        """í•™ìŠµ ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”"""
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        history_df = pd.DataFrame(history.history)
        history_df.to_csv(save_folder / 'training_history.csv', index=False)
        
        # í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Accuracy
        axes[0, 0].plot(history.history['accuracy'], label='Train')
        axes[0, 0].plot(history.history['val_accuracy'], label='Validation')
        axes[0, 0].set_title('Model Accuracy')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].legend()
        
        # Loss
        axes[0, 1].plot(history.history['loss'], label='Train')
        axes[0, 1].plot(history.history['val_loss'], label='Validation')
        axes[0, 1].set_title('Model Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        
        # Precision
        axes[1, 0].plot(history.history['precision'], label='Train')
        axes[1, 0].plot(history.history['val_precision'], label='Validation')
        axes[1, 0].set_title('Model Precision')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].legend()
        
        # Recall
        axes[1, 1].plot(history.history['recall'], label='Train')
        axes[1, 1].plot(history.history['val_recall'], label='Validation')
        axes[1, 1].set_title('Model Recall')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(save_folder / 'training_curves.png', dpi=300)
        plt.close()

def evaluate_model(model, val_generator, class_map, save_folder):
    """ëª¨ë¸ í‰ê°€ ë° ê²°ê³¼ ì €ì¥"""
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = model.predict(val_generator, verbose=1)
    y_pred = np.argmax(predictions, axis=1)
    y_true = val_generator.classes
    
    # í´ë˜ìŠ¤ ì´ë¦„ ëª©ë¡
    class_names = list(class_map.keys())
    
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸
    print("\n=== ì§ˆë³‘ ê·¸ë£¹ ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ===")
    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)
    print(report)
    
    # ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì €ì¥
    with open(save_folder / 'classification_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Confusion Matrix ì‹œê°í™”
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names,
                yticklabels=class_names)
    plt.title('Confusion Matrix - ì§ˆë³‘ ê·¸ë£¹ ë¶„ë¥˜')
    plt.xlabel('ì˜ˆì¸¡ëœ í´ë˜ìŠ¤')
    plt.ylabel('ì‹¤ì œ í´ë˜ìŠ¤')
    plt.tight_layout()
    plt.savefig(save_folder / 'confusion_matrix.png', dpi=300)
    plt.close()
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    accuracy = np.sum(y_pred == y_true) / len(y_true)
    print(f"\nì „ì²´ ì •í™•ë„: {accuracy:.4f}")

def save_model_files(model, save_folder, class_map):
    """ìµœì¢… ëª¨ë¸ íŒŒì¼ë“¤ ì €ì¥"""
    print("\nğŸ’¾ ëª¨ë¸ íŒŒì¼ ì €ì¥ ì¤‘...")
    
    # 1. H5 í˜•ì‹ìœ¼ë¡œ ì €ì¥
    h5_path = MODEL_DIR / 'eye_disease_model.h5'
    model.save(str(h5_path), save_format='h5')
    print(f"âœ… H5 ëª¨ë¸ ì €ì¥: {h5_path}")
    
    # 2. SavedModel í˜•ì‹ìœ¼ë¡œë„ ì €ì¥
    saved_model_path = MODEL_DIR / 'eye_disease_saved_model'
    model.save(str(saved_model_path))
    print(f"âœ… SavedModel ì €ì¥: {saved_model_path}")
    
    # 3. í´ë˜ìŠ¤ ë§¤í•‘ ì €ì¥ (JSON í˜•ì‹)
    class_map_for_json = {str(v): k for k, v in class_map.items()}
    with open(MODEL_DIR / 'class_map.json', 'w', encoding='utf-8') as f:
        json.dump(class_map_for_json, f, ensure_ascii=False, indent=2)
    print("âœ… í´ë˜ìŠ¤ ë§µ ì €ì¥")
    
    # 4. í•™ìŠµ ì •ë³´ ì €ì¥
    training_info = {
        "training_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "model_architecture": "EfficientNetB0",
        "input_shape": [IMG_SIZE, IMG_SIZE, 3],
        "num_classes": len(class_map),
        "preprocessing": "Division by 255.0",
        "framework": "TensorFlow " + tf.__version__,
        "platform": "Windows CPU",
        "disease_groups": list(class_map.keys()),
        "training_parameters": {
            "epochs": EPOCHS,
            "batch_size": BATCH_SIZE,
            "learning_rate": LEARNING_RATE,
            "optimizer": "Adam",
            "loss": "categorical_crossentropy"
        }
    }
    
    with open(MODEL_DIR / 'model_info.json', 'w', encoding='utf-8') as f:
        json.dump(training_info, f, ensure_ascii=False, indent=2)
    print("âœ… ëª¨ë¸ ì •ë³´ ì €ì¥")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¾ DuoPet AI ì•ˆêµ¬ì§ˆí™˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Windows ë²„ì „)")
    print("=" * 70)
    
    # ë°ì´í„°ì…‹ í™•ì¸
    if not DATASET_PATH.exists():
        print(f"âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATASET_PATH}")
        return
    
    # ë°ì´í„° ì²˜ë¦¬
    processor = DataProcessor(TRAIN_DIR, VAL_DIR)
    train_df, val_df = processor.prepare_data()
    
    if train_df.empty or val_df.empty:
        print("ğŸš¨ ì˜¤ë¥˜: í•™ìŠµ ë˜ëŠ” ê²€ì¦ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ë¶„í¬ ì¶œë ¥
    print(f"\ní•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ")
    print(f"ì§ˆë³‘ ê·¸ë£¹ ìˆ˜: {len(CLASS_MAP)}")
    print(f"ê·¸ë£¹ ëª©ë¡: {list(CLASS_MAP.keys())}")
    
    print("\nğŸ“Š ê·¸ë£¹ë³„ ë°ì´í„° ë¶„í¬ (í•™ìŠµ ë°ì´í„°):")
    print(train_df['label'].value_counts())
    
    print("\nğŸ“Š ê·¸ë£¹ë³„ ë°ì´í„° ë¶„í¬ (ê²€ì¦ ë°ì´í„°):")
    print(val_df['label'].value_counts())
    
    # ëª¨ë¸ í•™ìŠµ
    pipeline = ModelPipeline(train_df, val_df, CLASS_MAP, EPOCHS)
    model, save_folder = pipeline.train()
    
    # ëª¨ë¸ í‰ê°€
    _, val_gen = pipeline.get_data_generators()
    evaluate_model(model, val_gen, CLASS_MAP, save_folder)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    save_model_files(model, save_folder, CLASS_MAP)
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {MODEL_DIR}")
    print(f"ìƒì„¸ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {save_folder}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    K.clear_session()

if __name__ == "__main__":
    # GPU ë¹„í™œì„±í™” (CPUë§Œ ì‚¬ìš©)
    tf.config.set_visible_devices([], 'GPU')
    
    # CPU ìŠ¤ë ˆë“œ ìµœì í™”
    tf.config.threading.set_inter_op_parallelism_threads(4)
    tf.config.threading.set_intra_op_parallelism_threads(4)
    
    main()