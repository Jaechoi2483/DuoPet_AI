# --------------------------------------------------------------------------
# íŒŒì¼ëª…: services/chatbot/predict.py
# ì„¤ëª…: ë‹µë³€ ìƒì„± ë° ì˜ˆìƒ ì§ˆë¬¸ ìƒì„± ë¡œì§ ê°œì„  (LLM í”„ë¡¬í”„íŠ¸ ìµœì í™” ë° ë¹„ë™ê¸° ì²˜ë¦¬)
# --------------------------------------------------------------------------
import os
import json
import httpx
import functools
# import time # ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
import chromadb
from bs4 import BeautifulSoup, NavigableString
from openai import AsyncOpenAI  # ğŸ’¡ ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¡œ ë³€ê²½
from keybert import KeyBERT
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from typing import List, Dict, Any
from kospellcheck import SpellChecker
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from common.logger import get_logger  # ë¡œê¹…ì„ ìœ„í•´ ì¶”ê°€

# logger ê°ì²´ë¥¼ ëª¨ë“ˆ ì „ì—­ì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ì •ì˜í•©ë‹ˆë‹¤.
logger = get_logger(__name__)

global_chat_cache = {}

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì‘ ìˆ˜ì • ---
load_dotenv()

# httpx.Client ë™ê¸° í´ë¼ì´ì–¸íŠ¸ í”„ë¡ì‹œ íŒ¨ì¹˜
original_init = httpx.Client.__init__


@functools.wraps(original_init)
def patched_init(self, *args, **kwargs):
    if 'proxies' in kwargs:
        del kwargs['proxies']
    original_init(self, *args, **kwargs)


httpx.Client.__init__ = patched_init

# httpx.AsyncClient ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ í”„ë¡ì‹œ íŒ¨ì¹˜ (AsyncOpenAIê°€ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ì‹œ ì¶”ê°€)
original_async_init = httpx.AsyncClient.__init__


@functools.wraps(original_async_init)
def patched_async_init(self, *args, **kwargs):
    if 'proxies' in kwargs:
        del kwargs['proxies']
    original_async_init(self, *args, **kwargs)


httpx.AsyncClient.__init__ = patched_async_init

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ğŸš¨ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

try:
    # ğŸ’¡ ë¹„ë™ê¸° ìš”ì²­ì„ ìœ„í•´ AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (í˜„ì¬ ì½”ë“œë¥¼ ë”°ë¦„)
    client = AsyncOpenAI(api_key=api_key)
except Exception as e:
    raise RuntimeError(f"ğŸš¨ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# --- RAG ì±—ë´‡ í´ë˜ìŠ¤ ì •ì˜ ---
class RAGChatbot:
    def __init__(self, site_url: str, max_crawl_pages: int = 10):
        logger.info("ğŸ¤– RAG ì±—ë´‡ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")  # print -> logger.info
        self.site_url = site_url
        self.site_functions = [
            {"name": "notice_board", "description": "ê³µì§€ì‚¬í•­ í™•ì¸í•˜ê¸°", "url": "/notice"},
            {"name": "free_board", "description": "ììœ ê²Œì‹œíŒ ê°€ê¸°", "url": "/board"},
            {"name": "health_check", "description": "ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§„ë‹¨í•˜ê¸°", "url": "/health-check"},
            {"name": "behavior_analysis", "description": "ì´ìƒí–‰ë™ ë¶„ì„ ì„œë¹„ìŠ¤ ë³´ê¸°", "url": "/behavior-analysis"},
            {"name": "video_recommend", "description": "ì¶”ì²œ ì˜ìƒ ë³´ëŸ¬ê°€ê¸°", "url": "/recommendations"},
            {"name": "qna", "description": "qna", "url": "/qna"},
            {"name": "login", "description": "ë¡œê·¸ì¸", "url": "/login"}
        ]

        self.keyword_redirect_map = {
            "ê¶ê¸ˆ": ["qna", "faq", "free_board"],
            "ì§ˆë¬¸": ["qna", "faq", "free_board"],
            "ì•„íŒŒ": ["health_check", "behavior_analysis"],
            "ì§„ë‹¨": ["health_check", "behavior_analysis"],
            "ë°©ë²•": ["notice_board", "faq"],
            "ë¡œê·¸ì¸": ["login"],
            "ê°€ì…": ["login"],
            "ì‹¬ì‹¬": ["video_recommend", "free_board"]
        }

        self.base_url = f"{urlparse(self.site_url).scheme}://{urlparse(self.site_url).netloc}"
        self.max_crawl_pages = max_crawl_pages

        self.excluded_paths = [


            '/adoption'

        ]

        logger.info("KeyBERT ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")  # print -> logger.info
        self.kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')
        logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")  # print -> logger.info

        self.chroma_db_path = os.environ.get("CHROMA_DB_PATH", "./chroma_data")
        self.db_collection = self._setup_vector_db()

        if self.db_collection.count() == 0:
            logger.warning("âš ï¸ ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")  # print -> logger.warning
            self.knowledge_base = self._create_kb_from_site()
            if not self.knowledge_base:
                raise RuntimeError("ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLê³¼ ì‚¬ì´íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

            logger.info(f"--- ğŸ§  í¬ë¡¤ë§ëœ ì§€ì‹ {len(self.knowledge_base)}ê°œë¥¼ ë²¡í„° DBì— ì €ì¥ ì¤‘ ---")  # print -> logger.info
            self.db_collection.add(
                documents=[doc['content'] for doc in self.knowledge_base],
                metadatas=[doc['metadata'] for doc in self.knowledge_base],
                ids=[doc['id'] for doc in self.knowledge_base]
            )
            logger.info(f"âœ… ì´ {self.db_collection.count()}ê°œì˜ ì§€ì‹ì´ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")  # print -> logger.info
        else:
            logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° DBì—ì„œ {self.db_collection.count()}ê°œì˜ ì§€ì‹ ë¡œë”© ì™„ë£Œ. í¬ë¡¤ë§ì„ ê±´ë„ˆëœ€.")  # print -> logger.info
            self.knowledge_base = []

    def _get_page_content(self, url: str) -> str:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--log-level=3')
        options.add_argument('--window-size=1920,1080')

        driver = None
        try:
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)

            logger.info(f" [Selenium] '{url}' í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")  # print -> logger.info
            driver.get(url)

            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                logger.info(" [Selenium] í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì„±ê³µ.")  # print -> logger.info
            except Exception as wait_e:
                logger.warning(f" [Selenium] í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ: {wait_e}")  # print -> logger.warning

            html_content = driver.page_source

            # ë””ë²„ê¹…ìš© HTML ì¶œë ¥ ë° íŒŒì¼ ì €ì¥ (í•„ìš” ì—†ë‹¤ë©´ ì œê±°)
            logger.debug(f"\n--- ê°€ì ¸ì˜¨ HTML ì½˜í…ì¸  (ìƒìœ„ 500ì) ---\n{html_content[:500]}...\n---")  # print -> logger.debug
            # with open("crawled_page_content.html", "w", encoding="utf-8") as f:
            #     f.write(html_content)
            # logger.debug(f"ğŸ’¡ ê°€ì ¸ì˜¨ HTML ì½˜í…ì¸ ë¥¼ 'crawled_page_content.html' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.") # print -> logger.debug

            return html_content
        except Exception as e:
            logger.error(f"ğŸš¨ '{url}' í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)  # print -> logger.error
            return ""
        finally:
            if driver:
                driver.quit()

        # RAGChatbot í´ë˜ìŠ¤ ë‚´ë¶€ì— ë‹¤ë¥¸ ë©”ì†Œë“œë“¤ê³¼ í•¨ê»˜ ìœ„ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

    def _create_kb_from_site(self) -> List[Dict[str, Any]]:
        logger.info(f"--- ğŸŒ ì‚¬ì´íŠ¸ ì „ì²´ ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘ (ìµœëŒ€ {self.max_crawl_pages} í˜ì´ì§€) ---")

        urls_to_visit = {self.site_url}
        visited_urls = set()
        knowledge_base = []

        while urls_to_visit and len(visited_urls) < self.max_crawl_pages:
            current_url = urls_to_visit.pop()
            if current_url in visited_urls:
                continue

            logger.info(f"\n[í¬ë¡¤ë§ ì‹œì‘] -> {current_url}")
            visited_urls.add(current_url)
            html_content = self._get_page_content(current_url)
            if not html_content:
                logger.warning(" [ê²°ê³¼] í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            soup = BeautifulSoup(html_content, 'html.parser')
            page_title = soup.title.string.strip() if soup.title else 'ì œëª© ì—†ìŒ'
            logger.info(f" [í˜ì´ì§€ ì œëª©] {page_title}")

            content_area = soup.find('main') or soup.find('article') or soup.find('body')
            if not content_area:
                logger.warning(" [ê²°ê³¼] ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ bodyì—ì„œ ì¶”ì¶œ ì‹œë„.")
                content_area = soup.body

            chunks_from_page = []
            for element in content_area.find_all(
                    ['h1', 'h2', 'h3', 'h4', 'p', 'div', 'li', 'span', 'a', 'strong', 'em', 'dd', 'dt'],
                    recursive=True
            ):
                if isinstance(element, NavigableString): continue
                text = element.get_text(separator=' ', strip=True)
                if len(text) > 15 and '\n' not in text and 'function' not in text.lower() and 'var' not in text.lower():
                    chunks_from_page.append(text)

            unique_chunks = list(dict.fromkeys(chunks_from_page))

            for i, chunk in enumerate(unique_chunks):
                tags_list = []
                try:
                    tags_list = [kw for kw, score in self.kw_model.extract_keywords(chunk, top_n=3)]
                except Exception as e:
                    logger.warning(f"KeyBERT íƒœê·¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (í…ìŠ¤íŠ¸: '{chunk[:30]}...'): {e}")
                    tags_list = []

                tags_text = " ".join(tags_list)
                content_with_tags = f"{chunk}\n\n[TAGS: {tags_text}]"

                knowledge_base.append({
                    "id": f"{urlparse(current_url).path.replace('/', '_')}_{i}",
                    "content": content_with_tags,
                    "metadata": {"source": current_url, "title": page_title}
                })

            logger.info(f" [ì¶”ì¶œëœ ì •ë³´] {len(unique_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°")

            found_links = set()
            for link in content_area.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(self.base_url, href)
                parsed_link = urlparse(full_url)
                if (full_url.startswith(self.base_url) and
                        full_url not in visited_urls and
                        parsed_link.path not in self.excluded_paths and  # ğŸ‘ˆ ì œì™¸ ëª©ë¡ í™•ì¸
                        not parsed_link.fragment and
                        not (parsed_link.path.endswith(
                            ('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.xml', '.txt', '.pdf')))):
                    found_links.add(full_url)

            logger.info(f" [ë°œê²¬ëœ ë§í¬] {len(found_links)}ê°œ")
            urls_to_visit.update(found_links)

        if knowledge_base:
            logger.info(
                f"\nâœ… ì´ {len(knowledge_base)}ê°œì˜ ì§€ì‹ ë©ì–´ë¦¬ë¥¼ {len(visited_urls)}ê°œ í˜ì´ì§€ì—ì„œ ìµœì¢… ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return knowledge_base

    def _setup_vector_db(self) -> chromadb.Collection:
        chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
        collection_name = "chatbot_content_v5"

        try:
            collection = chroma_client.get_or_create_collection(name=collection_name)
            logger.info(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œ ë˜ëŠ” ìƒì„± ì„±ê³µ.")  # print -> logger.info
        except Exception as e:
            logger.warning(f"âš ï¸ ë²¡í„° DB ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")  # print -> logger.warning
            collection = chroma_client.create_collection(name=collection_name)
        return collection

    def resync_data_from_site(self):
        try:
            logger.info("ğŸ”„ ê´€ë¦¬ì ìš”ì²­: ì±—ë´‡ ë°ì´í„° ì „ì²´ ë¦¬í”„ë ˆì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")  # print -> logger.info

            current_count = self.db_collection.count()
            if current_count > 0:
                logger.info(f" - ê¸°ì¡´ ë°ì´í„° {current_count}ê°œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")  # print -> logger.info
                all_ids = self.db_collection.get(include=[])['ids']
                if all_ids:
                    self.db_collection.delete(ids=all_ids)
                logger.info(f" - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ. í˜„ì¬ ì¹´ìš´íŠ¸: {self.db_collection.count()}")  # print -> logger.info

            logger.info(" - ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")  # print -> logger.info
            new_knowledge_base = self._create_kb_from_site()
            if not new_knowledge_base:
                logger.error("ğŸš¨ ë¦¬í”„ë ˆì‹œ ì¤‘ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")  # print -> logger.error
                return

            logger.info(f" - ìƒˆë¡œìš´ ì§€ì‹ {len(new_knowledge_base)}ê°œë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤...")  # print -> logger.info
            self.db_collection.add(
                documents=[doc['content'] for doc in new_knowledge_base],
                metadatas=[doc['metadata'] for doc in new_knowledge_base],
                ids=[doc['id'] for doc in new_knowledge_base]
            )

            final_count = self.db_collection.count()
            logger.info(f"âœ… ì±—ë´‡ ë°ì´í„° ë¦¬í”„ë ˆì‹œ ì„±ê³µ! ì´ {final_count}ê°œì˜ ì§€ì‹ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")  # print -> logger.info

        except Exception as e:
            logger.error(f"ğŸš¨ ë°ì´í„° ë¦¬í”„ë ˆì‹œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)  # print -> logger.error

    def _check_for_keyword_redirect(self, query: str) -> Dict[str, Any] | None:
        detected_actions = set()
        for keyword, actions in self.keyword_redirect_map.items():
            if keyword in query:
                for action in actions:
                    detected_actions.add(action)
        if not detected_actions:
            return None
        return {
            "answer": "í˜¹ì‹œ ì´ëŸ° ê¸°ëŠ¥ë“¤ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”? ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì´ë™í•´ ë³´ì„¸ìš”.",
            "suggested_actions": list(detected_actions),
            "predicted_questions": []
        }

    def _hybrid_retrieve(self, query: str, n_results: int = 5) -> str:
        """
        KeyBERTë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , ë©”íƒ€ë°ì´í„° í•„í„°ë§ê³¼ ì‹œë§¨í‹± ê²€ìƒ‰ì„ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if self.db_collection.count() == 0:
            return ""

        try:
            # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•„í„°ë§ì— ì‚¬ìš©)
            keywords = [keyword for keyword, score in self.kw_model.extract_keywords(query, top_n=3)]
            logger.info(f" KeyBERT ì¶”ì¶œ í‚¤ì›Œë“œ: {keywords}")
        except Exception as e:
            logger.error(f"ğŸš¨ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            keywords = []

        # ğŸ‘‡ [ì¶”ê°€] ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ChromaDB where í•„í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        where_document_filter = None
        if keywords:
            # ë¬¸ì„œ ë‚´ìš©(content)ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ìƒ‰($contains)
            where_document_filter = {
                "$or": [{"$contains": keyword} for keyword in keywords]
            }

        enhanced_query = query + " " + " ".join(keywords)
        logger.info(f" ê°•í™”ëœ ê²€ìƒ‰ì–´: {enhanced_query}")

        # ğŸ‘‡ [ìˆ˜ì •] db_collection.query í˜¸ì¶œ ì‹œ where í•„í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        semantic_results = self.db_collection.query(
            query_texts=[enhanced_query],
            n_results=n_results,
            where_document=where_document_filter
        )

        docs_with_metadata = []
        if semantic_results and semantic_results['documents']:
            for i, doc in enumerate(semantic_results['documents'][0]):
                metadata = semantic_results['metadatas'][0][i]
                clean_doc = doc.split("\n\n[TAGS:")[0]
                docs_with_metadata.append(f"[ì¶œì²˜: {metadata.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc}")
        final_context = "\n\n".join(docs_with_metadata)
        logger.info(f"ChromaDB ê²€ìƒ‰ ê²°ê³¼ (RAG ì»¨í…ìŠ¤íŠ¸):\n---\n{final_context if final_context else 'ê²€ìƒ‰ëœ ê²°ê³¼ ì—†ìŒ.'}\n---")
        return final_context

    async def _generate_answer_and_actions(self, query: str, context: str, user_profile: Dict[str, Any],
                                           history: List[Dict[str, str]]) -> Dict[str, Any]:
        """ë‹µë³€ ë° ì¶”ì²œ ì•¡ì…˜, ì˜ˆìƒ ì§ˆë¬¸ì„ í•¨ê»˜ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
        user_display_name = user_profile.get('nickname', user_profile.get('name', 'ë¹„íšŒì›'))

        # ğŸ’¡ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë” ìƒì„¸íˆ í¬í•¨ì‹œí‚¤ê¸°
        user_profile_string_parts = []
        if user_profile.get('user_id'):
            user_profile_string_parts.append(f"ì‚¬ìš©ì ID: {user_profile['user_id']}")
        if user_profile.get('name'):
            user_profile_string_parts.append(f"ì´ë¦„: {user_profile['name']}")
        if user_profile.get('nickname'):
            user_profile_string_parts.append(f"ë‹‰ë„¤ì„: {user_profile['nickname']}")
        if user_profile.get('email'):
            user_profile_string_parts.append(f"ì´ë©”ì¼: {user_profile['email']}")
        if user_profile.get('member_since'):
            user_profile_string_parts.append(f"ê°€ì…ì¼: {user_profile['member_since']}")
        if user_profile.get('age'):
            user_profile_string_parts.append(f"ë‚˜ì´: {user_profile['age']}ì„¸")
        if user_profile.get('gender'):
            user_profile_string_parts.append(f"ì„±ë³„: {user_profile['gender']}")
        if user_profile.get('phone'):
            user_profile_string_parts.append(f"ì „í™”ë²ˆí˜¸: {user_profile['phone']}")
        if user_profile.get('address'):
            user_profile_string_parts.append(f"ì£¼ì†Œ: {user_profile['address']}")

        if user_profile.get('pet_info'):
            for i, pet in enumerate(user_profile['pet_info']):
                pet_details = (
                    f"ë°˜ë ¤ë™ë¬¼ {i + 1}: "
                    f"ì´ë¦„ {pet.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}, "
                    f"ì¢…ë¥˜ {pet.get('species', 'ì•Œ ìˆ˜ ì—†ìŒ')}, "
                    f"í’ˆì¢… {pet.get('breed', 'ì•Œ ìˆ˜ ì—†ìŒ')}, "
                    f"ë‚˜ì´ {pet.get('age', 'ì•Œ ìˆ˜ ì—†ìŒ')}ì„¸, "
                    f"ì„±ë³„ {pet.get('gender', 'ì•Œ ìˆ˜ ì—†ìŒ')}, "
                    f"ì¤‘ì„±í™” ì—¬ë¶€: {pet.get('neutered', 'ì•Œ ìˆ˜ ì—†ìŒ')}, "
                    f"ì²´ì¤‘: {pet.get('weight', 'ì•Œ ìˆ˜ ì—†ìŒ')}kg"  # 'kg' ë‹¨ìœ„ ì¶”ê°€
                )
                if pet.get('medical_history'): pet_details += f", íŠ¹ì´ì‚¬í•­: {pet['medical_history']}"
                if pet.get('registration_date'): pet_details += f", ë“±ë¡ì¼: {pet['registration_date']}"
                user_profile_string_parts.append(pet_details)
        else:
            user_profile_string_parts.append("ë°˜ë ¤ë™ë¬¼ ì •ë³´: ì—†ìŒ")

        if user_profile.get('role'):
            user_profile_string_parts.append(f"ì‚¬ìš©ì ì‹œìŠ¤í…œ ì—­í• : {user_profile['role']}")

        user_profile_string = "\n".join(user_profile_string_parts) if user_profile_string_parts else "ì—†ìŒ"

        functions_string = json.dumps(self.site_functions, indent=2, ensure_ascii=False)
        history_string = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])

        prompt = f"""
        ë‹¹ì‹ ì€ 'DuoPet' ì„œë¹„ìŠ¤ì˜ ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ì '{user_display_name}'ë‹˜ì„ ë„ì™€ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì‹­ì‹œì˜¤.

        **ì§€ì‹œì‚¬í•­:**

        1.  **ì •ë³´ ê¸°ë°˜ ë‹µë³€:** ì•„ë˜ [ì°¸ê³  ì •ë³´]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ [í˜„ì¬ ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ìœ¼ì‹­ì‹œì˜¤.
            -   ë§Œì•½ ê´€ë ¨ ì •ë³´ê°€ ìˆë‹¤ë©´, ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            -   ì°¸ê³  ì •ë³´ì— ë°˜ë ¤ë™ë¬¼ ì´ë¦„ì´ ìˆë‹¤ë©´, **ì§ˆë¬¸ê³¼ ê´€ë ¨ë  ê²½ìš° ë°˜ë“œì‹œ ë‹µë³€ì— ì •í™•íˆ í¬í•¨í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰**í•´ì•¼ í•©ë‹ˆë‹¤.

        2.  **ê°œì¸í™”ëœ ë‹µë³€:**
            -   **ì•„ë˜ [ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ê°œì¸í™”í•˜ì‹­ì‹œì˜¤.**
            -   íŠ¹íˆ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” í•´ë‹¹ ë°˜ë ¤ë™ë¬¼ì˜ ì´ë¦„, ì¢…, ë‚˜ì´ ë“±ì„ ì–¸ê¸‰í•˜ë©° ë” êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            -   ì‚¬ìš©ìì˜ ê³¼ê±° í™œë™ì´ë‚˜ ì„ í˜¸ë„ì— ê¸°ë°˜í•˜ì—¬ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ê¸°ëŠ¥ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.
            -   **ì‚¬ìš©ì í”„ë¡œí•„ì˜ 'ì‚¬ìš©ì ì‹œìŠ¤í…œ ì—­í• ' ì •ë³´(ì˜ˆ: ê´€ë¦¬ì)ë¥¼ ë‹µë³€ì— ì§ì ‘ì ì¸ í˜¸ì¹­ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.** ì˜¤ì§ ì‚¬ìš©ì '{user_display_name}'ë‹˜ë§Œì„ í˜¸ì¹­ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.

        3.  **ì¼ë°˜ ì§€ì‹ í™œìš©:**
            -   ë§Œì•½ [ì°¸ê³  ì •ë³´]ì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ì—†ë‹¤ë©´, ê·¸ë•ŒëŠ” ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            -   "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ëŠ” ë§ ëŒ€ì‹ , ë„ì›€ì´ ë˜ëŠ” ì¼ë°˜ì ì¸ ì¡°ì–¸ì´ë‚˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

        4.  **ê¸°ëŠ¥ ë° ì§ˆë¬¸ ì œì•ˆ:**
            -   ë‹µë³€ í›„, ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ê¸°ëŠ¥ì„ [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]ì—ì„œ ì°¾ì•„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.
            -   ì‚¬ìš©ìê°€ ë‹¤ìŒì— ê¶ê¸ˆí•´í•  ë§Œí•œ **ê´€ë ¨ í›„ì† ì§ˆë¬¸ 3ê°€ì§€**ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ìƒì„±í•˜ì‹­ì‹œì˜¤.
                - ì˜ˆìƒ ì§ˆë¬¸ ìƒì„± ì‹œì—ë„ ì‚¬ìš©ì í”„ë¡œí•„(íŠ¹íˆ ë°˜ë ¤ë™ë¬¼ ì •ë³´)ì„ í™œìš©í•˜ì—¬ ê°œì¸í™”ëœ ì§ˆë¬¸ì„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.

        5.  **ì¶œë ¥ í˜•ì‹:** ìµœì¢… ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

        **JSON ì¶œë ¥ í˜•ì‹:**
        {{
          "answer": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
          "suggested_actions": ["ê°€ì¥ ê´€ë ¨ìˆëŠ” í•¨ìˆ˜ì˜ name"],
          "predicted_questions": ["ì˜ˆìƒ ì§ˆë¬¸ 1", "ì˜ˆìƒ ì§ˆë¬¸ 2", "ì˜ˆìƒ ì§ˆë¬¸ 3"]
        }}

        ---
        [ì°¸ê³  ì •ë³´]
        {context if context else "ì—†ìŒ"}
        ---
        [ì´ì „ ëŒ€í™” ë‚´ìš©]
        {history_string if history_string else "ì—†ìŒ"}
        ---
        [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]
        {functions_string}
        ---
        [ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
        {user_profile_string}
        ---
        [í˜„ì¬ ì§ˆë¬¸]
        {query}
        ---

        JSON ì¶œë ¥:
        """
        try:
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system",
                     "content": f"ë‹¹ì‹ ì€ '{self.site_url}' ì›¹ì‚¬ì´íŠ¸ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë©°, ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°œì¸í™”ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            logger.error(f"ğŸš¨ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", exc_info=True)  # print -> logger.error
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, AI ëª¨ë¸ê³¼ í†µì‹ í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "suggested_actions": [], "predicted_questions": []}

    async def _is_context_relevant(self, query: str, context: str) -> bool:
        """RAGë¡œ ê²€ìƒ‰ëœ ì •ë³´ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ AIë¥¼ í†µí•´ í™•ì¸í•©ë‹ˆë‹¤."""
        # ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ í™•ì¸í•  í•„ìš” ì—†ì´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        if not context:
            return False

        # AIì—ê²Œ ë³´ë‚¼ ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸
        prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

        ê²€ìƒ‰ëœ ì •ë³´: "{context}"

        ìœ„ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ì •ë³´ê°€ ì„œë¡œ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? ì˜¤ì§ 'ë„¤' ë˜ëŠ” 'ì•„ë‹ˆì˜¤' í•œ ë‹¨ì–´ë¡œë§Œ ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.
        """
        try:
            response = await client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system",
                     "content": "You are an assistant that determines if a context is relevant to a query. Answer only with 'ë„¤' or 'ì•„ë‹ˆì˜¤'."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,  # ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
                max_tokens=5  # 'ë„¤' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë§Œ ë°›ê¸° ìœ„í•´ í† í° ìˆ˜ ì œí•œ
            )
            answer = response.choices[0].message.content.strip()
            logger.info(f"RAG ê´€ë ¨ì„± ê²€ì‚¬ ê²°ê³¼: '{answer}'")
            # ì‘ë‹µì— 'ë„¤'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            return "ë„¤" in answer
        except Exception as e:
            logger.error(f"RAG ê´€ë ¨ì„± ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ, ì•ˆì „í•˜ê²Œ ê´€ë ¨ ì—†ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
            return False

    # ask í•¨ìˆ˜ ì „ì²´ë¥¼ ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ êµì²´í•©ë‹ˆë‹¤.

    async def ask(self, query: str, user_profile: Dict[str, Any], history: List[Dict[str, str]] = []) -> Dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë°±ì—”ë“œ í•„í„°ë§ ì œê±°, í”„ë¡ íŠ¸ì—”ë“œ ì œì–´ë¡œ ë³€ê²½)"""

        # ë§ì¶¤ë²• ê²€ì‚¬ ë° ìºì‹œ í™•ì¸ (ê¸°ì¡´ê³¼ ë™ì¼)
        try:
            spell_checker = SpellChecker()
            corrected_query = spell_checker.check_spelling(query)['corrected_text']
            if query != corrected_query: logger.info(f"[ë§ì¶¤ë²• êµì •] '{query}' -> '{corrected_query}'")
        except Exception as e:
            logger.error(f"ğŸš¨ ë§ì¶¤ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
            corrected_query = query

        if corrected_query in global_chat_cache:
            logger.info(f"ìºì‹œ íˆíŠ¸(Cache Hit)!: '{corrected_query}'")
            return global_chat_cache[corrected_query]

        # í‚¤ì›Œë“œ ë˜ëŠ” AIë¥¼ í†µí•´ ì‘ë‹µ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
        response_json = self._check_for_keyword_redirect(corrected_query)
        if not response_json:
            context = self._hybrid_retrieve(corrected_query)
            response_json = await self._generate_answer_and_actions(corrected_query, context, user_profile, history)

        # ğŸ‘‡ [ìˆ˜ì •] ë¡œê·¸ì¸ ìƒíƒœ í•„í„°ë§ ë¡œì§ì„ ëª¨ë‘ ì œê±°í•˜ê³ ,
        # ë‹¨ìˆœíˆ ëª¨ë“  ì¶”ì²œ ì•¡ì…˜ì˜ ìƒì„¸ ì •ë³´ë§Œ ìƒì„±í•©ë‹ˆë‹¤.
        action_details = []
        if "suggested_actions" in response_json and isinstance(response_json["suggested_actions"], list):
            valid_action_names = {func['name'] for func in self.site_functions}
            for name in response_json["suggested_actions"]:
                if name.strip() in valid_action_names:
                    for func in self.site_functions:
                        if func['name'] == name.strip():
                            action_details.append({"name": func['name'], "description": func['description'],
                                                   "url": f"{self.base_url}{func['url']}"})
                            break

        response_json["suggested_actions"] = action_details

        if "predicted_questions" not in response_json:
            response_json["predicted_questions"] = []

        # ìºì‹œ ì €ì¥ ë° ë°˜í™˜
        global_chat_cache[corrected_query] = response_json
        return response_json