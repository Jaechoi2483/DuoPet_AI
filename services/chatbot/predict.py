# --------------------------------------------------------------------------
# íŒŒì¼ëª…: services/chatbot/predict.py
# ì„¤ëª…: ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡ì— ê³µì§€ì‚¬í•­ ë° ììœ ê²Œì‹œíŒì„ ì¶”ê°€í•˜ì—¬ ì•ˆë‚´ ê¸°ëŠ¥ ê°•í™”
# --------------------------------------------------------------------------
import os
import json
import httpx
import functools
import time
import chromadb
from bs4 import BeautifulSoup, NavigableString
from openai import OpenAI
from keybert import KeyBERT
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from typing import List, Dict, Any
from kospellcheck import SpellChecker
# ğŸ’¡ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°ë¥¼ ìœ„í•œ ì¶”ê°€ ì„í¬íŠ¸ (selenium.webdriver.support)
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì‘ ìˆ˜ì • ---
load_dotenv()

original_init = httpx.Client.__init__


@functools.wraps(original_init)
def patched_init(self, *args, **kwargs):
    if 'proxies' in kwargs:
        del kwargs['proxies']
    original_init(self, *args, **kwargs)


httpx.Client.__init__ = patched_init

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ğŸš¨ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

try:
    client = OpenAI(api_key=api_key)
except Exception as e:
    raise RuntimeError(f"ğŸš¨ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# --- RAG ì±—ë´‡ í´ë˜ìŠ¤ ì •ì˜ ---
class RAGChatbot:
    def __init__(self, site_url: str, max_crawl_pages: int = 10):
        print("ğŸ¤– RAG ì±—ë´‡ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        self.site_url = site_url
        self.site_functions = [
            {"name": "notice_board", "description": "ê³µì§€ì‚¬í•­ í™•ì¸í•˜ê¸°", "url": "/notice"},
            {"name": "free_board", "description": "ììœ ê²Œì‹œíŒ ê°€ê¸°", "url": "/board"},
            {"name": "health_check", "description": "ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§„ë‹¨í•˜ê¸°", "url": "/health-check"},
            {"name": "behavior_analysis", "description": "ì´ìƒí–‰ë™ ë¶„ì„ ì„œë¹„ìŠ¤ ë³´ê¸°", "url": "/behavior-analysis"},
            {"name": "video_recommend", "description": "ì¶”ì²œ ì˜ìƒ ë³´ëŸ¬ê°€ê¸°", "url": "/recommendations"},
            {"name": "qna", "description": "qna", "url": "/qna"},
            {"name": "login", "description": "ë¡œê·¸ì¸", "url": "/login"}
        ]

        self.keyword_redirect_map = {
            "ê¶ê¸ˆ": ["qna", "faq", "free_board"],
            "ì§ˆë¬¸": ["qna", "faq", "free_board"],
            "ì•„íŒŒ": ["health_check", "behavior_analysis"],
            "ì§„ë‹¨": ["health_check", "behavior_analysis"],
            "ë°©ë²•": ["notice_board", "faq"],
            "ë¡œê·¸ì¸": ["login"],
            "ê°€ì…": ["login"],
            "ì‹¬ì‹¬": ["video_recommend", "free_board"]
        }

        self.predefined_questions = {
            "notice_board": [
                "ìµœê·¼ ê³µì§€ì‚¬í•­ 3ê°œë§Œ ì•Œë ¤ì¤˜",
                "ì„œë¹„ìŠ¤ ì ê²€ ì¼ì •ì€ ì–¸ì œì•¼?"
            ],
            "free_board": [
                "ì‚¬ëŒë“¤ì´ ê°€ì¥ ë§ì´ ë³¸ ê¸€ì€ ë­ì•¼?",
                "ê°•ì•„ì§€ ìë‘ ê²Œì‹œíŒì€ ì–´ë””ì•¼?",
                "ê¸€ì„ ì“°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•´?"
            ],
            "health_check": [
                "ìš°ë¦¬ {pet_species} {pet_name}ê°€(ì´) ìê¾¸ ê·€ë¥¼ ê¸ì–´", # í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
                "ìš°ë¦¬ ì•„ì´ê°€ ì˜¤ëŠ˜ë”°ë¼ ê¸°ìš´ì´ ì—†ì–´",
                "ê±´ê°• ì§„ë‹¨ ê²°ê³¼ëŠ” ì €ì¥ë¼?"
            ],
            "behavior_analysis": [
                "ê°•ì•„ì§€ê°€ ê¼¬ë¦¬ë¥¼ ë¬´ëŠ” ì´ìœ ëŠ” ë­ì•¼?",
                "ê³ ì–‘ì´ê°€ ë°¤ì— ë„ˆë¬´ ì‹œë„ëŸ½ê²Œ ìš¸ì–´",
                "ë¶„ë¦¬ë¶ˆì•ˆ ì¦ìƒì— ëŒ€í•´ ì•Œë ¤ì¤˜"
            ],
            "video_recommend": [
                "ì˜¤ëŠ˜ì˜ ì¶”ì²œ ì˜ìƒ ë³´ì—¬ì¤˜",
                "ê°•ì•„ì§€ í›ˆë ¨ ê´€ë ¨ ì˜ìƒ ìˆì–´?",
                "ì¬ë¯¸ìˆëŠ” ë™ë¬¼ ì˜ìƒ ì¶”ì²œí•´ì¤˜"
            ],
            "qna": [
                "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì€ ë­ê°€ ìˆì–´?",
                "ê²°ì œ ê´€ë ¨í•´ì„œ ì§ˆë¬¸í•˜ê³  ì‹¶ì–´",
                "ë‚´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ì–´ë””ì„œ ë´?"
            ],
            # ì¶”ì²œ ê¸°ëŠ¥ì´ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê¸°ë³¸ ì§ˆë¬¸
            "default": [
                "{pet_age}ì‚´ì¸ ìš°ë¦¬ {pet_name}ì—ê²Œ ë§ëŠ” ì‚¬ë£Œ ì¶”ì²œí•´ì¤˜",  # í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
                "ìš°ë¦¬ {pet_species}ê°€ ì¢‹ì•„í•  ë§Œí•œ ì¥ë‚œê° ìˆì–´?",  # í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
                "ê°€ì¥ ì¸ê¸° ìˆëŠ” ì„œë¹„ìŠ¤ëŠ” ë­ì•¼?"
            ]
        }
        self.base_url = f"{urlparse(self.site_url).scheme}://{urlparse(self.site_url).netloc}"
        self.max_crawl_pages = max_crawl_pages

        print("KeyBERT ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        self.kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

        # ğŸ’¡ ë²¡í„° DB ì„¤ì • ë° ë°ì´í„° ë¡œë”© ë˜ëŠ” í¬ë¡¤ë§
        # ChromaDB ë°ì´í„°ê°€ ì €ì¥ë  ê²½ë¡œ ì„¤ì • (ì˜ˆ: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ 'chroma_data' í´ë”)
        self.chroma_db_path = os.environ.get("CHROMA_DB_PATH", "./chroma_data")  # .env íŒŒì¼ì—ì„œ ì„¤ì •í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        self.db_collection = self._setup_vector_db()  # ì»¬ë ‰ì…˜ ë¡œë“œ ë˜ëŠ” ìƒì„±

        # ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆë‹¤ë©´ í¬ë¡¤ë§ ë° ì €ì¥
        if self.db_collection.count() == 0:
            print("âš ï¸ ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.knowledge_base = self._create_kb_from_site()
            if not self.knowledge_base:
                # í¬ë¡¤ë§ í›„ì—ë„ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ê°„ì£¼
                raise RuntimeError("ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLê³¼ ì‚¬ì´íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

            # í¬ë¡¤ë§ëœ ì§€ì‹ì„ DBì— ì¶”ê°€ (ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš°)
            print(f"--- ğŸ§  í¬ë¡¤ë§ëœ ì§€ì‹ {len(self.knowledge_base)}ê°œë¥¼ ë²¡í„° DBì— ì €ì¥ ì¤‘ ---")
            self.db_collection.add(
                documents=[doc['content'] for doc in self.knowledge_base],
                metadatas=[doc['metadata'] for doc in self.knowledge_base],
                ids=[doc['id'] for doc in self.knowledge_base]
            )
            print(f"âœ… ì´ {self.db_collection.count()}ê°œì˜ ì§€ì‹ì´ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… ê¸°ì¡´ ë²¡í„° DBì—ì„œ {self.db_collection.count()}ê°œì˜ ì§€ì‹ ë¡œë”© ì™„ë£Œ. í¬ë¡¤ë§ì„ ê±´ë„ˆëœ€.")
            # knowledge_base ë³€ìˆ˜ëŠ” _hybrid_retrieve ë“±ì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
            # DBì—ì„œ ë¡œë“œí•  í•„ìš”ê°€ ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë‘ê±°ë‚˜ í•„ìš”ì— ë”°ë¼ ì ì ˆíˆ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            self.knowledge_base = []

    def _get_page_content(self, url: str) -> str:
        """Seleniumì„ ì‚¬ìš©í•´ ë‹¨ì¼ í˜ì´ì§€ì˜ HTML ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--log-level=3')
        options.add_argument('--window-size=1920,1080')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì—ì„œ ì°½ í¬ê¸° ì§€ì • (ì¼ë¶€ í˜ì´ì§€ ë Œë”ë§ì— ì˜í–¥)

        driver = None
        try:
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)

            print(f"  [Selenium] '{url}' í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
            driver.get(url)

            # ğŸ’¡ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œë¥¼ ìœ„í•œ ëª…ì‹œì  ëŒ€ê¸° ì¡°ê±´ ì¶”ê°€ (ì´ì „ ë‹µë³€ì—ì„œ ì¶”ê°€ëœ ë¶€ë¶„)
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                print("  [Selenium] í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì„±ê³µ.")
            except Exception as wait_e:
                print(f"  [Selenium] í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ: {wait_e}")
                # ê·¸ë˜ë„ page_sourceëŠ” ì‹œë„í•´ ë³¼ ìˆ˜ ìˆìŒ

            html_content = driver.page_source

            # ğŸ’¡ ê°€ì ¸ì˜¨ HTML ì½˜í…ì¸ ë¥¼ ì¶œë ¥í•˜ê³  íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹…ìš©, í•„ìš” ì—†ë‹¤ë©´ ì œê±°)
            print(f"\n--- ê°€ì ¸ì˜¨ HTML ì½˜í…ì¸  (ìƒìœ„ 500ì) ---\n{html_content[:500]}...\n---")
            with open("crawled_page_content.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"ğŸ’¡ ê°€ì ¸ì˜¨ HTML ì½˜í…ì¸ ë¥¼ 'crawled_page_content.html' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

            return html_content
        except Exception as e:
            print(f"ğŸš¨ '{url}' í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
        finally:
            if driver:
                driver.quit()

    def _create_kb_from_site(self) -> List[Dict[str, Any]]:
        """ì‚¬ì´íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ì—¬ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìƒì„¸ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"--- ğŸŒ ì‚¬ì´íŠ¸ ì „ì²´ ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘ (ìµœëŒ€ {self.max_crawl_pages} í˜ì´ì§€) ---")

        urls_to_visit = {self.site_url}
        visited_urls = set()
        knowledge_base = []

        while urls_to_visit and len(visited_urls) < self.max_crawl_pages:
            current_url = urls_to_visit.pop()
            if current_url in visited_urls:
                continue

            print(f"\n[í¬ë¡¤ë§ ì‹œì‘] -> {current_url}")
            visited_urls.add(current_url)
            html_content = self._get_page_content(current_url)
            if not html_content:
                print("  [ê²°ê³¼] í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            soup = BeautifulSoup(html_content, 'html.parser')
            page_title = soup.title.string.strip() if soup.title else 'ì œëª© ì—†ìŒ'
            print(f"  [í˜ì´ì§€ ì œëª©] {page_title}")

            # ğŸ’¡ ì½˜í…ì¸  ì˜ì—­ íƒìƒ‰ íƒœê·¸ í™•ì¥ (ì´ì „ ë””ë²„ê¹… ì¡°ì–¸ì— ë”°ë¦„)
            content_area = soup.find('main') or soup.find('article') or soup.find('body')
            if not content_area:  # bodyê°€ fallbackìœ¼ë¡œ ì§€ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ ì¡°ê±´ì€ ì‹¤ì œë¡œ bodyê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ ì‘ë™
                print("  [ê²°ê³¼] ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ bodyì—ì„œ ì¶”ì¶œ ì‹œë„.")
                content_area = soup.body  # ëª…ì‹œì ìœ¼ë¡œ bodyë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½

            chunks_from_page = []
            # ğŸ’¡ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  íƒœê·¸ ëª©ë¡ì„ í™•ì¥
            for element in content_area.find_all(
                    ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 'span', 'a', 'strong', 'em', 'dd', 'dt'],
                    # íƒœê·¸ í™•ì¥
                    recursive=True
            ):
                if isinstance(element, NavigableString): continue
                text = element.get_text(separator=' ', strip=True)
                # ğŸ’¡ ê¸¸ì´ ì œí•œ ì™„í™” ë° ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ í•„í„°ë§ ê°•í™”
                if len(text) > 15 and '\n' not in text and 'function' not in text.lower() and 'var' not in text.lower():
                    # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ë‚˜ JS ì½”ë“œì²˜ëŸ¼ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ í•„í„°ë§
                    chunks_from_page.append(text)

            unique_chunks = list(dict.fromkeys(chunks_from_page))

            for i, chunk in enumerate(unique_chunks):
                knowledge_base.append({
                    "id": f"{urlparse(current_url).path.replace('/', '_')}_{i}",
                    "content": chunk,
                    "metadata": {"source": current_url, "title": page_title}
                })

            print(f"  [ì¶”ì¶œëœ ì •ë³´] {len(unique_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°")

            found_links = set()
            for link in content_area.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(self.base_url, href)
                # ğŸ’¡ í˜„ì¬ ì‚¬ì´íŠ¸ URL ì‹œì‘ê³¼ ë™ì¼í•˜ê³ , ë°©ë¬¸í•˜ì§€ ì•Šì€ URLë§Œ ì¶”ê°€
                if full_url.startswith(self.base_url) and full_url not in visited_urls:
                    # ğŸ’¡ ë¶ˆí•„ìš”í•œ ì•µì»¤ ë§í¬ë‚˜ íŠ¹ì • íŒŒì¼ ë§í¬ëŠ” ê±´ë„ˆë›°ê¸° (ì¶”ê°€)
                    parsed_link = urlparse(full_url)
                    if not parsed_link.fragment and not (parsed_link.path.endswith(
                            ('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.xml', '.txt', '.pdf'))):
                        found_links.add(full_url)

            print(f"  [ë°œê²¬ëœ ë§í¬] {len(found_links)}ê°œ")
            urls_to_visit.update(found_links)

        if knowledge_base:
            print(f"\nâœ… ì´ {len(knowledge_base)}ê°œì˜ ì§€ì‹ ë©ì–´ë¦¬ë¥¼ {len(visited_urls)}ê°œ í˜ì´ì§€ì—ì„œ ìµœì¢… ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return knowledge_base

    def _setup_vector_db(self) -> chromadb.Collection:
        # ğŸ’¡ ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì˜êµ¬ì ì¸ ê²½ë¡œë¡œ ì´ˆê¸°í™”
        chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
        collection_name = "chatbot_content_v5"  # ì»¬ë ‰ì…˜ ì´ë¦„ ìœ ì§€

        try:
            # ğŸ’¡ ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            collection = chroma_client.get_or_create_collection(name=collection_name)  # get_or_create_collection ì‚¬ìš©
            print(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œ ë˜ëŠ” ìƒì„± ì„±ê³µ.")
        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒˆë¡œ ìƒì„± ì‹œë„
            print(f"âš ï¸ ë²¡í„° DB ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
            collection = chroma_client.create_collection(name=collection_name)

        # â—â—â— ì´ì œ ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„° ì¶”ê°€ëŠ” __init__ì—ì„œ ì¡°ê±´ì„ ê±¸ê³  ìˆ˜í–‰í•©ë‹ˆë‹¤.

        return collection

    def resync_data_from_site(self):
        """
        ê¸°ì¡´ ë²¡í„° DBì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³ , ì‚¬ì´íŠ¸ë¥¼ ìƒˆë¡œ í¬ë¡¤ë§í•˜ì—¬ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤.
        """
        try:
            print("ğŸ”„ ê´€ë¦¬ì ìš”ì²­: ì±—ë´‡ ë°ì´í„° ì „ì²´ ë¦¬í”„ë ˆì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

            # 1. ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ
            current_count = self.db_collection.count()
            if current_count > 0:
                print(f"  - ê¸°ì¡´ ë°ì´í„° {current_count}ê°œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
                # ChromaDBì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•˜ë ¤ë©´, ëª¨ë“  IDë¥¼ ê°€ì ¸ì™€ delete ë©”ì„œë“œì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
                all_ids = self.db_collection.get(include=[])['ids']
                if all_ids:
                    self.db_collection.delete(ids=all_ids)
                print(f"  - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ. í˜„ì¬ ì¹´ìš´íŠ¸: {self.db_collection.count()}")

            # 2. ì‚¬ì´íŠ¸ë¥¼ ìƒˆë¡œ í¬ë¡¤ë§í•˜ì—¬ ìƒˆë¡œìš´ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±
            print("  - ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
            new_knowledge_base = self._create_kb_from_site()
            if not new_knowledge_base:
                print("ğŸš¨ ë¦¬í”„ë ˆì‹œ ì¤‘ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # 3. ìƒˆë¡œìš´ ì§€ì‹ì„ ë²¡í„° DBì— ì¶”ê°€
            print(f"  - ìƒˆë¡œìš´ ì§€ì‹ {len(new_knowledge_base)}ê°œë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤...")
            self.db_collection.add(
                documents=[doc['content'] for doc in new_knowledge_base],
                metadatas=[doc['metadata'] for doc in new_knowledge_base],
                ids=[doc['id'] for doc in new_knowledge_base]
            )

            final_count = self.db_collection.count()
            print(f"âœ… ì±—ë´‡ ë°ì´í„° ë¦¬í”„ë ˆì‹œ ì„±ê³µ! ì´ {final_count}ê°œì˜ ì§€ì‹ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"ğŸš¨ ë°ì´í„° ë¦¬í”„ë ˆì‹œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")



    def _check_for_keyword_redirect(self, query: str) -> Dict[str, Any] | None:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ìˆë‹¤ë©´ ë¯¸ë¦¬ ì •ì˜ëœ ê¸°ëŠ¥ ì¶”ì²œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        detected_actions = set()
        for keyword, actions in self.keyword_redirect_map.items():
            if keyword in query:
                for action in actions:
                    detected_actions.add(action)

        if not detected_actions:
            return None  # ê°ì§€ëœ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜

        # ì¶”ì²œí•  ê¸°ëŠ¥ì˜ ìƒì„¸ ì •ë³´ë¥¼ self.site_functionsì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
        action_details = []
        for action_name in detected_actions:
            for func in self.site_functions:
                if func['name'] == action_name:
                    action_details.append({
                        "name": func['name'],
                        "description": func['description'],
                        "url": f"{self.base_url}{func['url']}"
                    })

        if not action_details:
            return None

        # ë¯¸ë¦¬ ì •ì˜ëœ ì‘ë‹µ JSONì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        return {
            "answer": "í˜¹ì‹œ ì´ëŸ° ê¸°ëŠ¥ë“¤ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”? ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì´ë™í•´ ë³´ì„¸ìš”.",
            "suggested_actions": action_details,
            "predicted_questions": []  # ë¹ ë¥¸ ì‘ë‹µì—ì„œëŠ” ì˜ˆìƒ ì§ˆë¬¸ì„ ë¹„ì›Œë‘¡ë‹ˆë‹¤.
        }

    def _hybrid_retrieve(self, query: str, n_results: int = 5) -> str:
        """
        [ìˆ˜ì •] KeyBERTë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ì‹œë§¨í‹± ê²€ìƒ‰ì„ í•¨ê»˜ ìˆ˜í–‰í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        if self.db_collection.count() == 0:
            return ""

        # 1. [ì¶”ê°€] KeyBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        # kw_model.extract_keywordsëŠ” (í‚¤ì›Œë“œ, ìœ ì‚¬ë„) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        try:
            keywords = [keyword for keyword, score in self.kw_model.extract_keywords(query, top_n=5)]
            print(f"  [ì¶”ì¶œëœ í‚¤ì›Œë“œ] {keywords}")
        except Exception as e:
            print(f"ğŸš¨ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            keywords = []

        # 2. [ìˆ˜ì •] ì›ë³¸ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œë¥¼ í•©ì³ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
        enhanced_query = query + " " + " ".join(keywords)
        print(f"  [ê°•í™”ëœ ê²€ìƒ‰ì–´] {enhanced_query}")

        # 3. ê°•í™”ëœ ê²€ìƒ‰ì–´ë¡œ ë²¡í„° DB ì¿¼ë¦¬
        semantic_results = self.db_collection.query(
            query_texts=[enhanced_query],  # ìˆ˜ì •ëœ ë¶€ë¶„
            n_results=n_results
        )

        docs_with_metadata = []
        if semantic_results and semantic_results['documents']:
            for i, doc in enumerate(semantic_results['documents'][0]):
                metadata = semantic_results['metadatas'][0][i]
                docs_with_metadata.append(f"[ì¶œì²˜: {metadata.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc}")

        return "\n\n".join(docs_with_metadata)

    def _generate_final_response(self, query: str, context: str, user_profile: Dict[str, Any],
                                 history: List[Dict[str, str]]) -> Dict[str, Any]:
        """ë‹¨ìˆœí•˜ê³  ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì— ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤."""
        # ë‹‰ë„¤ì„ì„ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì´ë¦„ì„ ì‚¬ìš©, ë‘˜ ë‹¤ ì—†ìœ¼ë©´ 'íšŒì›'ìœ¼ë¡œ ëŒ€ì²´
        user_display_name = user_profile.get('nickname', user_profile.get('name', 'íšŒì›'))

        functions_string = json.dumps(self.site_functions, indent=2, ensure_ascii=False)
        history_string = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])

        # ğŸ’¡ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë” ìƒì„¸íˆ í¬í•¨ì‹œí‚¤ê¸°
        user_profile_string_parts = []
        if user_profile.get('user_id'):
            user_profile_string_parts.append(f"ì‚¬ìš©ì ID: {user_profile['user_id']}")
        if user_profile.get('name'):
            user_profile_string_parts.append(f"ì´ë¦„: {user_profile['name']}")
        if user_profile.get('nickname'):
            user_profile_string_parts.append(f"ë‹‰ë„¤ì„: {user_profile['nickname']}")
        if user_profile.get('email'):
            user_profile_string_parts.append(f"ì´ë©”ì¼: {user_profile['email']}")
        if user_profile.get('member_since'):
            user_profile_string_parts.append(f"ê°€ì…ì¼: {user_profile['member_since']}")
        if user_profile.get('age'):
            user_profile_string_parts.append(f"ë‚˜ì´: {user_profile['age']}ì„¸")
        if user_profile.get('gender'):
            user_profile_string_parts.append(f"ì„±ë³„: {user_profile['gender']}")
        if user_profile.get('phone'):
            user_profile_string_parts.append(f"ì „í™”ë²ˆí˜¸: {user_profile['phone']}")
        if user_profile.get('address'):
            user_profile_string_parts.append(f"ì£¼ì†Œ: {user_profile['address']}")

        if user_profile.get('pet_info'):
            for i, pet in enumerate(user_profile['pet_info']):
                pet_details = (
                    f"ë°˜ë ¤ë™ë¬¼ {i + 1}: "
                    f"ì´ë¦„ {pet.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}, "
                    f"ì¢… {pet.get('species', 'ì•Œ ìˆ˜ ì—†ìŒ')}"
                )
                if pet.get('breed'): pet_details += f", í’ˆì¢… {pet['breed']}"
                if pet.get('age'): pet_details += f", ë‚˜ì´ {pet['age']}"
                if pet.get('gender'): pet_details += f", ì„±ë³„ {pet['gender']}"
                if pet.get('neutered') is not None: pet_details += f", ì¤‘ì„±í™” {pet['neutered']}"
                if pet.get('weight'): pet_details += f", ì²´ì¤‘ {pet['weight']}"
                if pet.get('medical_history'): pet_details += f", íŠ¹ì´ì‚¬í•­: {pet['medical_history']}"
                if pet.get('registration_date'): pet_details += f", ë“±ë¡ì¼: {pet['registration_date']}"
                user_profile_string_parts.append(pet_details)

        # ğŸš¨ 'ROLE' í•„ë“œëŠ” ì‚¬ìš©ì ì´ë¦„ê³¼ í˜¼ë™ë˜ì§€ ì•Šë„ë¡ ëª…í™•íˆ 'ì‚¬ìš©ì ì‹œìŠ¤í…œ ì—­í• 'ë¡œ ì§€ì¹­í•©ë‹ˆë‹¤.
        #    ë§Œì•½ ì´ ì •ë³´ê°€ ì±—ë´‡ì˜ ë‹µë³€ì— í•„ìš” ì—†ë‹¤ë©´, ì´ ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if user_profile.get('role'):
            user_profile_string_parts.append(f"ì‚¬ìš©ì ì‹œìŠ¤í…œ ì—­í• : {user_profile['role']}")

        user_profile_string = "\n".join(user_profile_string_parts) if user_profile_string_parts else "ì—†ìŒ"

        prompt = f"""
        ë‹¹ì‹ ì€ 'DuoPet' ì„œë¹„ìŠ¤ì˜ ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ì '{user_display_name}'ë‹˜ì„ ë„ì™€ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì‹­ì‹œì˜¤.

        **ì§€ì‹œì‚¬í•­:**

        1.  **ì •ë³´ ê¸°ë°˜ ë‹µë³€:** ì•„ë˜ [ì°¸ê³  ì •ë³´]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ [í˜„ì¬ ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ìœ¼ì‹­ì‹œì˜¤.
        2.  **ê°œì¸í™”ëœ ë‹µë³€:** ì•„ë˜ [ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ê°œì¸í™”í•˜ì‹­ì‹œì˜¤. íŠ¹íˆ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” í•´ë‹¹ ë°˜ë ¤ë™ë¬¼ì˜ ì´ë¦„, ì¢… ë“±ì„ ì–¸ê¸‰í•˜ë©° ë” êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
        3.  **ì¼ë°˜ ì§€ì‹ í™œìš©:** [ì°¸ê³  ì •ë³´]ì— ë‹µì´ ì—†ë‹¤ë©´, ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

        4.  **[í•„ìˆ˜] í›„ì† ì§ˆë¬¸ ì œì•ˆ:**
            -   ë‹µë³€ì´ ëë‚œ í›„, ì‚¬ìš©ìê°€ ë‹¤ìŒì— ê¶ê¸ˆí•´í•  ë§Œí•œ **ê´€ë ¨ í›„ì† ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ë°˜ë“œì‹œ ì˜ˆì¸¡í•˜ì—¬ ìƒì„±**í•´ì•¼ í•©ë‹ˆë‹¤.
            -   ì´ ì§ˆë¬¸ë“¤ì€ ì‚¬ìš©ì í”„ë¡œí•„(íŠ¹íˆ ë°˜ë ¤ë™ë¬¼ ì •ë³´)ì„ í™œìš©í•˜ì—¬ ê°œì¸í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            -   ì´ ì‘ì—…ì€ ì„ íƒ ì‚¬í•­ì´ ì•„ë‹ˆë©°, **ê²°ê³¼ JSONì— 'predicted_questions' í‚¤ê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.**

        5.  **[í•„ìˆ˜] ê¸°ëŠ¥ ì œì•ˆ ë° ì¶œë ¥ í˜•ì‹:**
            -   ë‹µë³€ê³¼ ê´€ë ¨ ìˆëŠ” ê¸°ëŠ¥ì„ [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]ì—ì„œ ì°¾ì•„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.
            -   ìµœì¢… ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, **ëª…ì‹œëœ ëª¨ë“  í‚¤ë¥¼ í¬í•¨**í•´ì•¼ í•©ë‹ˆë‹¤.

        **JSON ì¶œë ¥ í˜•ì‹:**
        {{
          "answer": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
          "suggested_actions": ["ê°€ì¥ ê´€ë ¨ìˆëŠ” í•¨ìˆ˜ì˜ name"],
          "predicted_questions": ["ì˜ˆìƒ ì§ˆë¬¸ 1", "ì˜ˆìƒ ì§ˆë¬¸ 2", "ì˜ˆìƒ ì§ˆë¬¸ 3"]
        }}

        ---
        [ì°¸ê³  ì •ë³´]
        {context if context else "ì—†ìŒ"}
        ---
        [ì´ì „ ëŒ€í™” ë‚´ìš©]
        {history_string if history_string else "ì—†ìŒ"}
        ---
        [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]
        {functions_string}
        ---
        [ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
        {user_profile_string}
        ---
        [í˜„ì¬ ì§ˆë¬¸]
        {query}
        ---

        JSON ì¶œë ¥:
        """
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system",
                     "content": f"ë‹¹ì‹ ì€ '{self.site_url}' ì›¹ì‚¬ì´íŠ¸ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë©°, ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°œì¸í™”ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"ğŸš¨ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, AI ëª¨ë¸ê³¼ í†µì‹ í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "suggested_actions": [], "predicted_questions": []}

    def ask(self, query: str, user_profile: Dict[str, Any], history: List[Dict[str, str]] = []) -> Dict[str, Any]:
        """
        ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜.
        [ìˆ˜ì •] ì‚¬ìš©ì ë¡œê·¸ì¸ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ì‘ë‹µ ë¡œì§ì„ ë¶„ê¸°í•©ë‹ˆë‹¤.
        """
        # 1. ì‚¬ìš©ì ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        is_logged_in = user_profile and user_profile.get('user_id') not in [None, '0']
        user_display_name = user_profile.get('nickname', 'ê³ ê°')

        # 2. ë¡œê·¸ì¸ ì‚¬ìš©ìì˜ 'ë¡œê·¸ì¸' ì§ˆë¬¸ì— ëŒ€í•œ ì¦‰ê°ì ì¸ ë‹µë³€
        if is_logged_in and any(keyword in query for keyword in ["ë¡œê·¸ì¸", "ê°€ì…"]):
            print(f"[ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸] '{user_display_name}'ë‹˜ì€ ì´ë¯¸ ë¡œê·¸ì¸ ìƒíƒœì…ë‹ˆë‹¤. í™•ì •ëœ ë‹µë³€ì„ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {
                "answer": f"{user_display_name}ë‹˜ì€ ì´ë¯¸ ë¡œê·¸ì¸ ìƒíƒœì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.",
                "suggested_actions": [
                    {"name": "free_board", "description": "ììœ ê²Œì‹œíŒ ê°€ê¸°", "url": f"{self.base_url}/board"},
                    {"name": "health_check", "description": "ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§„ë‹¨í•˜ê¸°", "url": f"{self.base_url}/health-check"}
                ],
                "predicted_questions": [
                    "ë‚´ ì •ë³´ëŠ” ì–´ë””ì„œ í™•ì¸í•´?",
                    "ìš°ë¦¬ ì•„ì´ ê±´ê°• ê¸°ë¡ ë³´ê³  ì‹¶ì–´",
                    "ììœ ê²Œì‹œíŒì— ë‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ë¬´ìŠ¨ ê¸€ì„ ì¼ì–´?"
                ]
            }

        # 3. try...finally êµ¬ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ëŠ¥ ëª©ë¡ì„ ì•ˆì „í•˜ê²Œ ì„ì‹œ ë³€ê²½ ë° ë³µì›
        original_functions = self.site_functions
        if is_logged_in:
            print("[ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸] ì¶”ì²œ ê¸°ëŠ¥ ëª©ë¡ì—ì„œ 'ë¡œê·¸ì¸'ì„ ì„ì‹œë¡œ ì œì™¸í•©ë‹ˆë‹¤.")
            self.site_functions = [func for func in original_functions if func['name'] != 'login']

        try:
            # --- ë§ì¶¤ë²• ê²€ì‚¬ ---
            try:
                spell_checker = SpellChecker()
                result_dict = spell_checker.check_spelling(query)
                corrected_query = result_dict['corrected_text']
                if query != corrected_query:
                    print(
                        f"\n[ë§ì¶¤ë²• êµì •] ì›ë³¸: '{query}' -> êµì •: '{corrected_query}' (ì˜¤ë¥˜ {result_dict.get('error_count', 0)}ê°œ)")
                else:
                    print(f"\n[ë§ì¶¤ë²• êµì •] ì›ë³¸ê³¼ ë™ì¼: '{query}'")
            except Exception as e:
                print(f"ğŸš¨ ë§ì¶¤ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©): '{e}'")
                corrected_query = query

            # --- í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ëŠ¥ ì¶”ì²œ ---
            keyword_response = self._check_for_keyword_redirect(corrected_query)
            if keyword_response:
                print(f"\n[í‚¤ì›Œë“œ ê°ì§€] '{corrected_query}'ì— ëŒ€í•œ ë¹ ë¥¸ ì‘ë‹µ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.")
                return keyword_response

            # --- RAG ë° LLM í˜¸ì¶œ ---
            context = self._hybrid_retrieve(corrected_query)
            print(f"\n[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]\n---\n{context}\n---")
            response_json = self._generate_final_response(corrected_query, context, user_profile, history)

            # --- ì¶”ì²œ ê¸°ëŠ¥(suggested_actions) ì •ë¦¬ ---
            if "suggested_actions" in response_json and isinstance(response_json["suggested_actions"], list):
                action_details = []
                valid_action_names = {func['name'] for func in self.site_functions}
                for action_name in response_json["suggested_actions"]:
                    if action_name in valid_action_names:
                        for func in self.site_functions:
                            if func['name'] == action_name:
                                action_details.append({
                                    "name": func['name'],
                                    "description": func['description'],
                                    "url": f"{self.base_url}{func['url']}"
                                })
                response_json["suggested_actions"] = action_details
            else:
                response_json["suggested_actions"] = []

            # --- ì˜ˆìƒ ì§ˆë¬¸(predicted_questions) ì„ íƒ ---
            selected_questions = []
            if response_json.get("suggested_actions"):
                first_action_name = response_json["suggested_actions"][0]['name']
                selected_questions = self.predefined_questions.get(first_action_name,
                                                                   self.predefined_questions['default'])
            else:
                selected_questions = self.predefined_questions['default']

            final_questions = []
            # ì‚¬ìš©ìì˜ ì²« ë²ˆì§¸ ë°˜ë ¤ë™ë¬¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜´ (ì—†ìœ¼ë©´ None)
            pet = user_profile['pet_info'][0] if user_profile.get('pet_info') else None

            for q_template in selected_questions:
                if pet:
                    # ë°˜ë ¤ë™ë¬¼ ì •ë³´ê°€ ìˆìœ¼ë©´, í…œí”Œë¦¿ì— ì •ë³´ë¥¼ ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.
                    # .format()ì€ KeyErrorë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, .replace()ë¥¼ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                    question = q_template.replace('{pet_name}', pet.get('name', 'ë°˜ë ¤ë™ë¬¼'))
                    question = question.replace('{pet_species}', pet.get('species', 'ë°˜ë ¤ë™ë¬¼'))
                    question = question.replace('{pet_age}', str(pet.get('age', 'Nì‚´')))  # ë‚˜ì´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
                    final_questions.append(question)
                else:
                    # ë°˜ë ¤ë™ë¬¼ ì •ë³´ê°€ ì—†ìœ¼ë©´, í…œí”Œë¦¿ ë³€ìˆ˜ë¥¼ ì¼ë°˜ì ì¸ ë‹¨ì–´ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
                    question = q_template.replace('{pet_name}', 'ë°˜ë ¤ë™ë¬¼')
                    question = question.replace('{pet_species}', 'ë°˜ë ¤ë™ë¬¼')
                    question = question.replace('{pet_age}', 'ìš°ë¦¬ ì•„ì´')
                    final_questions.append(question)

            # response_jsonì˜ predicted_questionsë¥¼ ìµœì¢… ì™„ì„±ëœ ì§ˆë¬¸ ëª©ë¡ìœ¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.
            response_json["predicted_questions"] = final_questions[:3]

            return response_json

        finally:
            # [ìˆ˜ì •] try ë¸”ë¡ì˜ ì‘ì—…ì´ ëë‚˜ë©´(ì„±ê³µ/ì‹¤íŒ¨ ë¬´ê´€) í•­ìƒ ì›ë˜ ê¸°ëŠ¥ ëª©ë¡ìœ¼ë¡œ ë³µì›
            self.site_functions = original_functions
            if is_logged_in:
                print("[ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ] ê¸°ëŠ¥ ëª©ë¡ì„ ì›ë˜ ìƒíƒœë¡œ ë³µì›í•©ë‹ˆë‹¤.")
