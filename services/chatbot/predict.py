import os
import json
import httpx
import functools
import time
import chromadb
import traceback
from bs4 import BeautifulSoup, NavigableString
from openai import OpenAI
from keybert import KeyBERT
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from typing import List, Dict, Any
from kospellcheck import SpellChecker
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì‘ ìˆ˜ì • ---
load_dotenv()

original_init = httpx.Client.__init__


@functools.wraps(original_init)
def patched_init(self, *args, **kwargs):
    if 'proxies' in kwargs:
        del kwargs['proxies']
    original_init(self, *args, **kwargs)


httpx.Client.__init__ = patched_init

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ğŸš¨ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

try:
    client = OpenAI(api_key=api_key)
except Exception as e:
    raise RuntimeError(f"ğŸš¨ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


class RAGChatbot:
    def __init__(self, site_url: str, max_crawl_pages: int = 50):
        print("ğŸ¤– RAG ì±—ë´‡ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        self.site_url = site_url
        self.site_functions = [
            {"name": "notice_board", "description": "ê³µì§€ì‚¬í•­ í™•ì¸í•˜ê¸°", "url": "/notice"},
            {"name": "free_board", "description": "ììœ ê²Œì‹œíŒ ê°€ê¸°", "url": "/community/freeBoard"},
            {"name": "health_check", "description": "ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§„ë‹¨í•˜ê¸°", "url": "/health/ai-diagnosis"},
            {"name": "behavior_analysis", "description": "ì´ìƒí–‰ë™ ë¶„ì„ ì„œë¹„ìŠ¤ ë³´ê¸°", "url": "/health/ai-behavior"},
            {"name": "qna", "description": "qna", "url": "/qna"},
            {"name": "login", "description": "ë¡œê·¸ì¸", "url": "/login"}
        ]

        self.keyword_redirect_map = {
            "ê¶ê¸ˆ": ["qna", "faq", "free_board"],
            "ì§ˆë¬¸": ["qna", "faq", "free_board"],
            "ì•„íŒŒ": ["health_check", "behavior_analysis"],
            "ì§„ë‹¨": ["health_check", "behavior_analysis"],
            "ë°©ë²•": ["notice_board", "faq"],
            "ë¡œê·¸ì¸": ["login"],
            "ê°€ì…": ["login"],
            "ì‹¬ì‹¬": ["video_recommend", "free_board"]
        }

        self.predefined_questions = {
            "notice_board": [
                "ìµœê·¼ ê³µì§€ì‚¬í•­ 3ê°œë§Œ ì•Œë ¤ì¤˜",
                "ì„œë¹„ìŠ¤ ì ê²€ ì¼ì •ì€ ì–¸ì œì•¼?"
            ],
            "free_board": [
                "ì‚¬ëŒë“¤ì´ ê°€ì¥ ë§ì´ ë³¸ ê¸€ì€ ë­ì•¼?",
                "ê°•ì•„ì§€ ìë‘ ê²Œì‹œíŒì€ ì–´ë””ì•¼?",
                "ê¸€ì„ ì“°ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•´?"
            ],
            "health_check": [
                "ìš°ë¦¬ {pet_species} {pet_name}ê°€(ì´) ìê¾¸ ê·€ë¥¼ ê¸ì–´",
                "ìš°ë¦¬ ì•„ì´ê°€ ì˜¤ëŠ˜ë”°ë¼ ê¸°ìš´ì´ ì—†ì–´",
                "ê±´ê°• ì§„ë‹¨ ê²°ê³¼ëŠ” ì €ì¥ë¼?"
            ],
            "behavior_analysis": [
                "ê°•ì•„ì§€ê°€ ê¼¬ë¦¬ë¥¼ ë¬´ëŠ” ì´ìœ ëŠ” ë­ì•¼?",
                "ê³ ì–‘ì´ê°€ ë°¤ì— ë„ˆë¬´ ì‹œë„ëŸ½ê²Œ ìš¸ì–´",
                "ë¶„ë¦¬ë¶ˆì•ˆ ì¦ìƒì— ëŒ€í•´ ì•Œë ¤ì¤˜"
            ],
            "qna": [
                "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì€ ë­ê°€ ìˆì–´?",
                "ê²°ì œ ê´€ë ¨í•´ì„œ ì§ˆë¬¸í•˜ê³  ì‹¶ì–´",
                "ë‚´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ì–´ë””ì„œ ë´?"
            ],
            "default": [
                "{pet_age}ì‚´ì¸ ìš°ë¦¬ {pet_name}ì—ê²Œ ë§ëŠ” ì‚¬ë£Œ ì¶”ì²œí•´ì¤˜",
                "ìš°ë¦¬ {pet_species}ê°€ ì¢‹ì•„í•  ë§Œí•œ ì¥ë‚œê° ìˆì–´?",
                "ê°€ì¥ ì¸ê¸° ìˆëŠ” ì„œë¹„ìŠ¤ëŠ” ë­ì•¼?"
            ]
        }
        self.base_url = f"{urlparse(self.site_url).scheme}://{urlparse(self.site_url).netloc}"
        self.max_crawl_pages = max_crawl_pages

        print("KeyBERT ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        self.kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

        self.chroma_db_path = os.environ.get("CHROMA_DB_PATH", "./api/chroma_data")
        self.db_collection = self._setup_vector_db()

        if self.db_collection.count() == 0:
            print("âš ï¸ ê¸°ì¡´ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.knowledge_base = self._create_kb_from_site()
            if not self.knowledge_base:
                raise RuntimeError("ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLê³¼ ì‚¬ì´íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

            print(f"--- ğŸ§  í¬ë¡¤ë§ëœ ì§€ì‹ {len(self.knowledge_base)}ê°œë¥¼ ë²¡í„° DBì— ì €ì¥ ì¤‘ ---")
            self.db_collection.add(
                documents=[doc['content'] for doc in self.knowledge_base],
                metadatas=[doc['metadata'] for doc in self.knowledge_base],
                ids=[doc['id'] for doc in self.knowledge_base]
            )
            print(f"âœ… ì´ {self.db_collection.count()}ê°œì˜ ì§€ì‹ì´ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… ê¸°ì¡´ ë²¡í„° DBì—ì„œ {self.db_collection.count()}ê°œì˜ ì§€ì‹ ë¡œë”© ì™„ë£Œ. í¬ë¡¤ë§ì„ ê±´ë„ˆëœ€.")
            self.knowledge_base = []

    def _get_page_content(self, url: str) -> str:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--log-level=3')
        options.add_argument('--window-size=1920,1080')
        options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36')

        driver = None
        try:
            print("  [Selenium] ChromeDriverë¥¼ ì„¤ì •í•˜ëŠ” ì¤‘...")
            driver_path = ChromeDriverManager().install()
            service = ChromeService(executable_path=driver_path)

            driver = webdriver.Chrome(service=service, options=options)
            print(f"  [Selenium] '{url}' í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
            driver.get(url)

            wait = WebDriverWait(driver, 15)
            wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div[class*='_container']"))
            )
            print("  [Selenium] í˜ì´ì§€ì˜ ì£¼ìš” ì½˜í…ì¸  ì»¨í…Œì´ë„ˆ(_container) ë¡œë”©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

            time.sleep(1)
            return driver.page_source
        except TimeoutException:
            print(f"  [Selenium ê²½ê³ ] '{url}' í˜ì´ì§€ì—ì„œ '_container'ë¥¼ ì‹œê°„ ë‚´ì— ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            return driver.page_source if driver else ""
        except Exception as e:
            print(f"ğŸš¨ '{url}' í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()
            return ""
        finally:
            if driver:
                driver.quit()

    def _create_kb_from_site(self) -> List[Dict[str, Any]]:
        print(f"--- ğŸŒ ì‚¬ì´íŠ¸ ì „ì²´ ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘ (ìµœëŒ€ {self.max_crawl_pages} í˜ì´ì§€) ---")
        urls_to_visit = {self.site_url}
        visited_urls = set()
        knowledge_base = []
        while urls_to_visit and len(visited_urls) < self.max_crawl_pages:
            current_url = urls_to_visit.pop()
            if current_url in visited_urls: continue
            print(f"\n[í¬ë¡¤ë§ ì‹œì‘] -> {current_url}")
            visited_urls.add(current_url)
            html_content = self._get_page_content(current_url)
            if not html_content:
                print("  [ê²°ê³¼] í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue
            soup = BeautifulSoup(html_content, 'html.parser')
            page_title = soup.title.string.strip() if soup.title else 'ì œëª© ì—†ìŒ'
            print(f"  [í˜ì´ì§€ ì œëª©] {page_title}")

            for tag in soup(['header', 'footer', 'nav', 'script', 'style']):
                tag.decompose()

            content_area = soup.select_one("div[class*='_container']") or soup.body

            if not content_area:
                print("  [ê²°ê³¼] ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            print(f"  [ì½˜í…ì¸  ì˜ì—­ íƒìƒ‰] ì„ íƒëœ ì˜ì—­: <{content_area.name} class='{' '.join(content_area.get('class', []))}'>")

            chunks_from_page = []

            for element in content_area.find_all(['h2', 'h3', 'p', 'td', 'li']):

                if isinstance(element, NavigableString): continue
                text = element.get_text(separator=' ', strip=True)
                if len(text.split()) > 3:
                    chunks_from_page.append(text)

            unique_chunks = list(dict.fromkeys(chunks_from_page))
            for i, chunk in enumerate(unique_chunks):
                knowledge_base.append({
                    "id": f"{urlparse(current_url).path.replace('/', '_')}_{i}",
                    "content": chunk,
                    "metadata": {"source": current_url, "title": page_title}
                })
            print(f"  [ì¶”ì¶œëœ ì •ë³´] {len(unique_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°")

            found_links = set()
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(self.base_url, href)
                if full_url.startswith(self.base_url) and full_url not in visited_urls:
                    parsed_link = urlparse(full_url)
                    if not parsed_link.fragment and not (parsed_link.path.endswith(
                            ('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.zip', '.pdf'))):
                        found_links.add(full_url)
            print(f"  [ë°œê²¬ëœ ë§í¬] {len(found_links)}ê°œ")
            urls_to_visit.update(found_links)
        if knowledge_base:
            print(f"\nâœ… ì´ {len(knowledge_base)}ê°œì˜ ì§€ì‹ ë©ì–´ë¦¬ë¥¼ {len(visited_urls)}ê°œ í˜ì´ì§€ì—ì„œ ìµœì¢… ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return knowledge_base

    def _setup_vector_db(self) -> chromadb.Collection:
        chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
        collection_name = "chatbot_content_v5"
        collection = chroma_client.get_or_create_collection(name=collection_name)
        print(f"âœ… ê¸°ì¡´ ë²¡í„° DB ì»¬ë ‰ì…˜ '{collection_name}' ë¡œë“œ ë˜ëŠ” ìƒì„± ì„±ê³µ.")
        return collection

    def resync_data_from_site(self):
        try:
            print("ğŸ”„ ê´€ë¦¬ì ìš”ì²­: ì±—ë´‡ ë°ì´í„° ì „ì²´ ë¦¬í”„ë ˆì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
            current_count = self.db_collection.count()
            if current_count > 0:
                print(f"  - ê¸°ì¡´ ë°ì´í„° {current_count}ê°œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
                all_ids = self.db_collection.get(include=[])['ids']
                if all_ids: self.db_collection.delete(ids=all_ids)
                print(f"  - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ. í˜„ì¬ ì¹´ìš´íŠ¸: {self.db_collection.count()}")
            print("  - ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
            new_knowledge_base = self._create_kb_from_site()
            if not new_knowledge_base:
                print("ğŸš¨ ë¦¬í”„ë ˆì‹œ ì¤‘ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return
            print(f"  - ìƒˆë¡œìš´ ì§€ì‹ {len(new_knowledge_base)}ê°œë¥¼ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤...")
            self.db_collection.add(
                documents=[doc['content'] for doc in new_knowledge_base],
                metadatas=[doc['metadata'] for doc in new_knowledge_base],
                ids=[doc['id'] for doc in new_knowledge_base]
            )
            final_count = self.db_collection.count()
            print(f"âœ… ì±—ë´‡ ë°ì´í„° ë¦¬í”„ë ˆì‹œ ì„±ê³µ! ì´ {final_count}ê°œì˜ ì§€ì‹ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"ğŸš¨ ë°ì´í„° ë¦¬í”„ë ˆì‹œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            traceback.print_exc()

    def _check_for_keyword_redirect(self, query: str) -> Dict[str, Any] | None:
        detected_actions = set()
        for keyword, actions in self.keyword_redirect_map.items():
            if keyword in query:
                for action in actions: detected_actions.add(action)
        if not detected_actions: return None
        action_details = []
        for action_name in detected_actions:
            for func in self.site_functions:
                if func['name'] == action_name:

                    action_details.append(
                        {"name": func['name'], "description": func['description'], "url": func['url']})
        if not action_details: return None
        return {"answer": "í˜¹ì‹œ ì´ëŸ° ê¸°ëŠ¥ë“¤ì„ ì°¾ê³  ê³„ì‹ ê°€ìš”? ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì´ë™í•´ ë³´ì„¸ìš”.", "suggested_actions": action_details,
                "predicted_questions": []}

    def _hybrid_retrieve(self, query: str, n_results: int = 5, source_filter: str = None) -> str:
        if self.db_collection.count() == 0: return ""

        try:
            keywords = [keyword for keyword, score in self.kw_model.extract_keywords(query, top_n=5)]
            print(f"  [ì¶”ì¶œëœ í‚¤ì›Œë“œ] {keywords}")
        except Exception as e:
            print(f"ğŸš¨ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            keywords = []

        enhanced_query = query + " " + " ".join(keywords)
        print(f"  [ê°•í™”ëœ ê²€ìƒ‰ì–´] {enhanced_query}")

        # ğŸ’¡ ê°œì„ : ChromaDBê°€ $likeë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ Pythonì—ì„œ ì§ì ‘ í•„í„°ë§í•©ë‹ˆë‹¤.
        # í•„í„°ë§ì„ ìœ„í•´ ì¶©ë¶„í•œ í›„ë³´êµ°ì„ í™•ë³´í•˜ë„ë¡ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤.
        query_n_results = 50 if source_filter else n_results

        query_params = {
            'query_texts': [enhanced_query],
            'n_results': query_n_results
        }

        # ğŸ’¡ ê°œì„ : where ì ˆì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # if source_filter:
        #     where_clause = {"source": {"$like": f"%{source_filter}%"}} # ì´ ë¶€ë¶„ì´ ì˜¤ë¥˜ì˜ ì›ì¸
        #     query_params['where'] = where_clause
        #     print(f"  [ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì ìš©] source: {source_filter}")

        semantic_results = self.db_collection.query(**query_params)

        docs_with_metadata = []
        if semantic_results and semantic_results['documents']:
            documents = semantic_results['documents'][0]
            metadatas = semantic_results['metadatas'][0]

            for i, doc in enumerate(documents):
                metadata = metadatas[i]

                # ğŸ’¡ ê°œì„ : source_filterê°€ ìˆëŠ” ê²½ìš°, Python ì½”ë“œ ë ˆë²¨ì—ì„œ í•„í„°ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                if source_filter:
                    if source_filter in metadata.get('source', ''):
                        docs_with_metadata.append(f"[ì¶œì²˜: {metadata.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc}")
                else:
                    docs_with_metadata.append(f"[ì¶œì²˜: {metadata.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc}")

        # ìµœì¢…ì ìœ¼ë¡œ ìš”ì²­ëœ ê°œìˆ˜(n_results)ë§Œí¼ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        print(f"  [í•„í„°ë§ í›„ ì»¨í…ìŠ¤íŠ¸] {len(docs_with_metadata)}ê°œ ë¬¸ì„œ ì„ íƒë¨ (ìµœëŒ€ {n_results}ê°œ ë°˜í™˜)")
        return "\n\n".join(docs_with_metadata[:n_results])

    def _generate_final_response(self, query: str, context: str, user_profile: Dict[str, Any],
                                 history: List[Dict[str, str]]) -> Dict[str, Any]:
        user_display_name = user_profile.get('nickname', user_profile.get('name', 'íšŒì›'))
        functions_string = json.dumps(self.site_functions, indent=2, ensure_ascii=False)
        history_string = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
        user_profile_string_parts = []
        if user_profile.get('user_id'): user_profile_string_parts.append(f"ì‚¬ìš©ì ID: {user_profile['user_id']}")
        if user_profile.get('name'): user_profile_string_parts.append(f"ì´ë¦„: {user_profile['name']}")
        if user_profile.get('nickname'): user_profile_string_parts.append(f"ë‹‰ë„¤ì„: {user_profile['nickname']}")
        if user_profile.get('email'): user_profile_string_parts.append(f"ì´ë©”ì¼: {user_profile['email']}")
        if user_profile.get('member_since'): user_profile_string_parts.append(f"ê°€ì…ì¼: {user_profile['member_since']}")
        if user_profile.get('age'): user_profile_string_parts.append(f"ë‚˜ì´: {user_profile['age']}ì„¸")
        if user_profile.get('gender'): user_profile_string_parts.append(f"ì„±ë³„: {user_profile['gender']}")
        if user_profile.get('phone'): user_profile_string_parts.append(f"ì „í™”ë²ˆí˜¸: {user_profile['phone']}")
        if user_profile.get('address'): user_profile_string_parts.append(f"ì£¼ì†Œ: {user_profile['address']}")
        if user_profile.get('pet_info'):
            for i, pet in enumerate(user_profile['pet_info']):
                pet_details = (f"ë°˜ë ¤ë™ë¬¼ {i + 1}: ì´ë¦„ {pet.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}, ì¢… {pet.get('species', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                if pet.get('breed'): pet_details += f", í’ˆì¢… {pet['breed']}"
                if pet.get('age'): pet_details += f", ë‚˜ì´ {pet['age']}"
                if pet.get('gender'): pet_details += f", ì„±ë³„ {pet['gender']}"
                if pet.get('neutered') is not None: pet_details += f", ì¤‘ì„±í™” {pet['neutered']}"
                if pet.get('weight'): pet_details += f", ì²´ì¤‘ {pet['weight']}"
                if pet.get('medical_history'): pet_details += f", íŠ¹ì´ì‚¬í•­: {pet['medical_history']}"
                if pet.get('registration_date'): pet_details += f", ë“±ë¡ì¼: {pet['registration_date']}"
                user_profile_string_parts.append(pet_details)
        if user_profile.get('role'): user_profile_string_parts.append(f"ì‚¬ìš©ì ì‹œìŠ¤í…œ ì—­í• : {user_profile['role']}")
        user_profile_string = "\n".join(user_profile_string_parts) if user_profile_string_parts else "ì—†ìŒ"
        prompt = f"""
        ë‹¹ì‹ ì€ 'DuoPet' ì„œë¹„ìŠ¤ì˜ ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ì '{user_display_name}'ë‹˜ì„ ë„ì™€ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì‹­ì‹œì˜¤.
        **ì§€ì‹œì‚¬í•­:**
        1.  **ì •ë³´ ê¸°ë°˜ ë‹µë³€:** ì•„ë˜ [ì°¸ê³  ì •ë³´]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ [í˜„ì¬ ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ìœ¼ì‹­ì‹œì˜¤.
        2.  **ê°œì¸í™”ëœ ë‹µë³€:** ì•„ë˜ [ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ê°œì¸í™”í•˜ì‹­ì‹œì˜¤. íŠ¹íˆ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” í•´ë‹¹ ë°˜ë ¤ë™ë¬¼ì˜ ì´ë¦„, ì¢… ë“±ì„ ì–¸ê¸‰í•˜ë©° ë” êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
        3.  **ì¼ë°˜ ì§€ì‹ í™œìš©:** [ì°¸ê³  ì •ë³´]ì— ë‹µì´ ì—†ë‹¤ë©´, ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
        4.  **[í•„ìˆ˜] í›„ì† ì§ˆë¬¸ ì œì•ˆ:**
            -   ë‹µë³€ì´ ëë‚œ í›„, ì‚¬ìš©ìê°€ ë‹¤ìŒì— ê¶ê¸ˆí•´í•  ë§Œí•œ **ê´€ë ¨ í›„ì† ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ë°˜ë“œì‹œ ì˜ˆì¸¡í•˜ì—¬ ìƒì„±**í•´ì•¼ í•©ë‹ˆë‹¤.
            -   ì´ ì§ˆë¬¸ë“¤ì€ ì‚¬ìš©ì í”„ë¡œí•„(íŠ¹íˆ ë°˜ë ¤ë™ë¬¼ ì •ë³´)ì„ í™œìš©í•˜ì—¬ ê°œì¸í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            -   ì´ ì‘ì—…ì€ ì„ íƒ ì‚¬í•­ì´ ì•„ë‹ˆë©°, **ê²°ê³¼ JSONì— 'predicted_questions' í‚¤ê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.**
        5.  **[í•„ìˆ˜] ê¸°ëŠ¥ ì œì•ˆ ë° ì¶œë ¥ í˜•ì‹:**
            -   ë‹µë³€ê³¼ ê´€ë ¨ ìˆëŠ” ê¸°ëŠ¥ì„ [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]ì—ì„œ ì°¾ì•„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.
            -   ìµœì¢… ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, **ëª…ì‹œëœ ëª¨ë“  í‚¤ë¥¼ í¬í•¨**í•´ì•¼ í•©ë‹ˆë‹¤.
        **JSON ì¶œë ¥ í˜•ì‹:**
        {{
          "answer": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
          "suggested_actions": ["ê°€ì¥ ê´€ë ¨ìˆëŠ” í•¨ìˆ˜ì˜ name"],
          "predicted_questions": ["ì˜ˆìƒ ì§ˆë¬¸ 1", "ì˜ˆìƒ ì§ˆë¬¸ 2", "ì˜ˆìƒ ì§ˆë¬¸ 3"]
        }}
        ---
        [ì°¸ê³  ì •ë³´]
        {context if context else "ì—†ìŒ"}
        ---
        [ì´ì „ ëŒ€í™” ë‚´ìš©]
        {history_string if history_string else "ì—†ìŒ"}
        ---
        [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]
        {functions_string}
        ---
        [ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
        {user_profile_string}
        ---
        [í˜„ì¬ ì§ˆë¬¸]
        {query}
        ---
        JSON ì¶œë ¥:
        """
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system",
                     "content": f"ë‹¹ì‹ ì€ '{self.site_url}' ì›¹ì‚¬ì´íŠ¸ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë©°, ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°œì¸í™”ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"ğŸš¨ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, AI ëª¨ë¸ê³¼ í†µì‹ í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "suggested_actions": [], "predicted_questions": []}

    def ask(self, query: str, user_profile: Dict[str, Any], history: List[Dict[str, str]] = []) -> Dict[str, Any]:
        is_logged_in = user_profile and user_profile.get('user_id') not in [None, '0']
        user_display_name = user_profile.get('nickname', 'ê³ ê°')
        if is_logged_in and any(keyword in query for keyword in ["ë¡œê·¸ì¸", "ê°€ì…"]):
            print(f"[ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸] '{user_display_name}'ë‹˜ì€ ì´ë¯¸ ë¡œê·¸ì¸ ìƒíƒœì…ë‹ˆë‹¤. í™•ì •ëœ ë‹µë³€ì„ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {
                "answer": f"{user_display_name}ë‹˜ì€ ì´ë¯¸ ë¡œê·¸ì¸ ìƒíƒœì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ í¸í•˜ê²Œ ë§ì”€í•´ì£¼ì„¸ìš”.",
                "suggested_actions": [
                    {"name": "free_board", "description": "ììœ ê²Œì‹œíŒ ê°€ê¸°", "url": f"{self.base_url}/board"},
                    {"name": "health_check", "description": "ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§„ë‹¨í•˜ê¸°", "url": f"{self.base_url}/health-check"}
                ],
                "predicted_questions": ["ë‚´ ì •ë³´ëŠ” ì–´ë””ì„œ í™•ì¸í•´?", "ìš°ë¦¬ ì•„ì´ ê±´ê°• ê¸°ë¡ ë³´ê³  ì‹¶ì–´", "ììœ ê²Œì‹œíŒì— ë‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ë¬´ìŠ¨ ê¸€ì„ ì¼ì–´?"]
            }
        original_functions = self.site_functions
        if is_logged_in:
            print("[ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸] ì¶”ì²œ ê¸°ëŠ¥ ëª©ë¡ì—ì„œ 'ë¡œê·¸ì¸'ì„ ì„ì‹œë¡œ ì œì™¸í•©ë‹ˆë‹¤.")
            self.site_functions = [func for func in original_functions if func['name'] != 'login']
        try:
            try:
                spell_checker = SpellChecker()
                result_dict = spell_checker.check_spelling(query)
                corrected_query = result_dict['corrected_text']
                if query != corrected_query:
                    print(
                        f"\n[ë§ì¶¤ë²• êµì •] ì›ë³¸: '{query}' -> êµì •: '{corrected_query}' (ì˜¤ë¥˜ {result_dict.get('error_count', 0)}ê°œ)")
                else:
                    print(f"\n[ë§ì¶¤ë²• êµì •] ì›ë³¸ê³¼ ë™ì¼: '{query}'")
            except Exception as e:
                print(f"ğŸš¨ ë§ì¶¤ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©): '{e}'")
                corrected_query = query
            keyword_response = self._check_for_keyword_redirect(corrected_query)
            if keyword_response:
                print(f"\n[í‚¤ì›Œë“œ ê°ì§€] '{corrected_query}'ì— ëŒ€í•œ ë¹ ë¥¸ ì‘ë‹µ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.")
                return keyword_response
            source_filter = None
            if "ê³µì§€" in corrected_query:
                source_filter = "/notice"
            elif "ììœ ê²Œì‹œíŒ" in corrected_query:
                source_filter = "/community/freeBoard"
            elif "ì§ˆë¬¸" in corrected_query or "QnA" in corrected_query.upper():
                source_filter = "/qna"
            context = self._hybrid_retrieve(corrected_query, source_filter=source_filter)
            print(f"\n[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]\n---\n{context if context else 'ê´€ë ¨ ì •ë³´ ì—†ìŒ'}\n---")
            response_json = self._generate_final_response(corrected_query, context, user_profile, history)
            if "suggested_actions" in response_json and isinstance(response_json["suggested_actions"], list):
                action_details = []
                valid_action_names = {func['name'] for func in self.site_functions}
                for action_name in response_json["suggested_actions"]:
                    if action_name in valid_action_names:
                        for func in self.site_functions:
                            if func['name'] == action_name:
                                action_details.append(
                                    {"name": func['name'], "description": func['description'], "url": func['url']})
                response_json["suggested_actions"] = action_details
            else:
                response_json["suggested_actions"] = []
            selected_questions = []
            if response_json.get("suggested_actions"):
                first_action_name = response_json["suggested_actions"][0]['name']
                selected_questions = self.predefined_questions.get(first_action_name,
                                                                   self.predefined_questions['default'])
            else:
                selected_questions = self.predefined_questions['default']
            final_questions = []
            pet = user_profile['pet_info'][0] if user_profile.get('pet_info') else None
            for q_template in selected_questions:
                if pet:
                    question = q_template.replace('{pet_name}', pet.get('name', 'ë°˜ë ¤ë™ë¬¼'))
                    question = question.replace('{pet_species}', pet.get('species', 'ë°˜ë ¤ë™ë¬¼'))
                    question = question.replace('{pet_age}', str(pet.get('age', 'Nì‚´')))
                    final_questions.append(question)
                else:
                    question = q_template.replace('{pet_name}', 'ë°˜ë ¤ë™ë¬¼')
                    question = question.replace('{pet_species}', 'ë°˜ë ¤ë™ë¬¼')
                    question = question.replace('{pet_age}', 'ìš°ë¦¬ ì•„ì´')
                    final_questions.append(question)
            response_json["predicted_questions"] = final_questions[:3]
            return response_json
        finally:
            self.site_functions = original_functions
            if is_logged_in:
                print("[ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ] ê¸°ëŠ¥ ëª©ë¡ì„ ì›ë˜ ìƒíƒœë¡œ ë³µì›í•©ë‹ˆë‹¤.")