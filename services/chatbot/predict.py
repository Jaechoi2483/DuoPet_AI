# --------------------------------------------------------------------------
# íŒŒì¼ëª…: services/chatbot/predict.py
# ì„¤ëª…: ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡ì— ê³µì§€ì‚¬í•­ ë° ììœ ê²Œì‹œíŒì„ ì¶”ê°€í•˜ì—¬ ì•ˆë‚´ ê¸°ëŠ¥ ê°•í™”
# --------------------------------------------------------------------------
import os
import json
import httpx
import functools
import time
import chromadb
from bs4 import BeautifulSoup, NavigableString
from openai import OpenAI
from keybert import KeyBERT
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from typing import List, Dict, Any

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë™ì‘ ìˆ˜ì • ---
load_dotenv()

original_init = httpx.Client.__init__


@functools.wraps(original_init)
def patched_init(self, *args, **kwargs):
    if 'proxies' in kwargs:
        del kwargs['proxies']
    original_init(self, *args, **kwargs)


httpx.Client.__init__ = patched_init

api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ğŸš¨ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

try:
    client = OpenAI(api_key=api_key)
except Exception as e:
    raise RuntimeError(f"ğŸš¨ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# --- RAG ì±—ë´‡ í´ë˜ìŠ¤ ì •ì˜ ---
class RAGChatbot:
    # â—â—â— __init__ ë©”ì„œë“œì— SITE_FUNCTIONSë¥¼ ì§ì ‘ ì •ì˜í•˜ë„ë¡ ìˆ˜ì • â—â—â—
    def __init__(self, site_url: str, max_crawl_pages: int = 10):
        print("ğŸ¤– RAG ì±—ë´‡ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        self.site_url = site_url
        # â— ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡ì„ í´ë˜ìŠ¤ ë‚´ë¶€ì— ì •ì˜í•©ë‹ˆë‹¤.
        self.site_functions = [
            {"name": "notice_board", "description": "ê³µì§€ì‚¬í•­ í™•ì¸í•˜ê¸°", "url": "/notice"},
            {"name": "free_board", "description": "ììœ ê²Œì‹œíŒ ê°€ê¸°", "url": "/board"},
            {"name": "health_check", "description": "ë°˜ë ¤ë™ë¬¼ ê±´ê°• ì§„ë‹¨í•˜ê¸°", "url": "/health-check"},
            {"name": "behavior_analysis", "description": "ì´ìƒí–‰ë™ ë¶„ì„ ì„œë¹„ìŠ¤ ë³´ê¸°", "url": "/behavior-analysis"},
            {"name": "video_recommend", "description": "ì¶”ì²œ ì˜ìƒ ë³´ëŸ¬ê°€ê¸°", "url": "/recommendations"}
        ]
        self.base_url = f"{urlparse(self.site_url).scheme}://{urlparse(self.site_url).netloc}"
        self.max_crawl_pages = max_crawl_pages

        print("KeyBERT ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")
        self.kw_model = KeyBERT('paraphrase-multilingual-MiniLM-L12-v2')
        print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

        self.knowledge_base = self._create_kb_from_site()
        if not self.knowledge_base:
            raise RuntimeError("ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLê³¼ ì‚¬ì´íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        self.db_collection = self._setup_vector_db()

    def _get_page_content(self, url: str) -> str:
        """Seleniumì„ ì‚¬ìš©í•´ ë‹¨ì¼ í˜ì´ì§€ì˜ HTML ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--log-level=3')
        driver = None
        try:
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            driver.get(url)
            time.sleep(3)
            return driver.page_source
        except Exception as e:
            print(f"ğŸš¨ '{url}' í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
        finally:
            if driver:
                driver.quit()

    def _create_kb_from_site(self) -> List[Dict[str, Any]]:
        """ì‚¬ì´íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ í¬ë¡¤ë§í•˜ì—¬ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê³  ìƒì„¸ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"--- ğŸŒ ì‚¬ì´íŠ¸ ì „ì²´ ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘ (ìµœëŒ€ {self.max_crawl_pages} í˜ì´ì§€) ---")

        urls_to_visit = {self.site_url}
        visited_urls = set()
        knowledge_base = []

        while urls_to_visit and len(visited_urls) < self.max_crawl_pages:
            current_url = urls_to_visit.pop()
            if current_url in visited_urls:
                continue

            print(f"\n[í¬ë¡¤ë§ ì‹œì‘] -> {current_url}")
            visited_urls.add(current_url)
            html_content = self._get_page_content(current_url)
            if not html_content:
                print("  [ê²°ê³¼] í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            soup = BeautifulSoup(html_content, 'html.parser')
            page_title = soup.title.string.strip() if soup.title else 'ì œëª© ì—†ìŒ'
            print(f"  [í˜ì´ì§€ ì œëª©] {page_title}")

            content_area = soup.find('main') or soup.find('article') or soup.find('body')
            if not content_area:
                print("  [ê²°ê³¼] ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                continue

            chunks_from_page = []
            for element in content_area.find_all(['h1', 'h2', 'h3', 'p', 'div', 'li', 'span', 'a'], recursive=True):
                if isinstance(element, NavigableString): continue
                text = element.get_text(separator=' ', strip=True)
                if len(text) > 20 and '\n' not in text:
                    chunks_from_page.append(text)

            unique_chunks = list(dict.fromkeys(chunks_from_page))

            for i, chunk in enumerate(unique_chunks):
                knowledge_base.append({
                    "id": f"{urlparse(current_url).path.replace('/', '_')}_{i}",
                    "content": chunk,
                    "metadata": {"source": current_url, "title": page_title}
                })

            print(f"  [ì¶”ì¶œëœ ì •ë³´] {len(unique_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°")

            found_links = set()
            for link in content_area.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(self.base_url, href)
                if full_url.startswith(self.base_url) and full_url not in visited_urls:
                    found_links.add(full_url)

            print(f"  [ë°œê²¬ëœ ë§í¬] {len(found_links)}ê°œ")
            urls_to_visit.update(found_links)

        if knowledge_base:
            print(f"\nâœ… ì´ {len(knowledge_base)}ê°œì˜ ì§€ì‹ ë©ì–´ë¦¬ë¥¼ {len(visited_urls)}ê°œ í˜ì´ì§€ì—ì„œ ìµœì¢… ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return knowledge_base

    def _setup_vector_db(self) -> chromadb.Collection:
        print("--- ğŸ§  ë²¡í„° DB ì„¤ì • ë° ì§€ì‹ ì €ì¥ ì‹œì‘ ---")
        chroma_client = chromadb.Client()
        collection_name = "chatbot_content_v5"
        try:
            chroma_client.delete_collection(name=collection_name)
        except Exception:
            pass

        collection = chroma_client.create_collection(name=collection_name)

        if not self.knowledge_base:
            print("âš ï¸ ì €ì¥í•  ì§€ì‹ì´ ì—†ì–´ ë²¡í„° DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return collection

        collection.add(
            documents=[doc['content'] for doc in self.knowledge_base],
            metadatas=[doc['metadata'] for doc in self.knowledge_base],
            ids=[doc['id'] for doc in self.knowledge_base]
        )
        print(f"âœ… ì´ {collection.count()}ê°œì˜ ì§€ì‹ì´ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return collection

    def _hybrid_retrieve(self, query: str, n_results: int = 5) -> str:
        """ì‹œë§¨í‹± ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if self.db_collection.count() == 0:
            return ""

        semantic_results = self.db_collection.query(query_texts=[query], n_results=n_results)

        docs_with_metadata = []
        for i, doc in enumerate(semantic_results['documents'][0]):
            metadata = semantic_results['metadatas'][0][i]
            docs_with_metadata.append(f"[ì¶œì²˜: {metadata.get('title', 'ì•Œ ìˆ˜ ì—†ìŒ')}]\n{doc}")

        return "\n\n".join(docs_with_metadata)

    def _generate_final_response(self, query: str, context: str, user_profile: Dict[str, Any],
                                 history: List[Dict[str, str]]) -> Dict[str, Any]:
        """ë‹¨ìˆœí•˜ê³  ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ LLMì— ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤."""
        user_name = user_profile.get('name', 'íšŒì›')
        functions_string = json.dumps(self.site_functions, indent=2, ensure_ascii=False)
        history_string = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])

        prompt = f"""
        ë‹¹ì‹ ì€ 'DuoPet' ì„œë¹„ìŠ¤ì˜ ìœ ëŠ¥í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‚¬ìš©ì '{user_name}'ë‹˜ì„ ë„ì™€ì£¼ì„¸ìš”.

        **ì§€ì‹œì‚¬í•­:**

        1.  **ì •ë³´ ê¸°ë°˜ ë‹µë³€:** ë¨¼ì €, ì•„ë˜ [ì°¸ê³  ì •ë³´]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ [í˜„ì¬ ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ìœ¼ì‹­ì‹œì˜¤.
            -   ë§Œì•½ ê´€ë ¨ ì •ë³´ê°€ ìˆë‹¤ë©´, ê·¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            -   ë‹µë³€ì€ í•­ìƒ '{user_name}ë‹˜, 'ìœ¼ë¡œ ì‹œì‘í•˜ì‹­ì‹œì˜¤.

        2.  **ì¼ë°˜ ì§€ì‹ í™œìš©:**
            -   ë§Œì•½ [ì°¸ê³  ì •ë³´]ì— ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ì—†ë‹¤ë©´, ê·¸ë•ŒëŠ” ë‹¹ì‹ ì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            -   "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ëŠ” ë§ ëŒ€ì‹ , ë„ì›€ì´ ë˜ëŠ” ì¼ë°˜ì ì¸ ì¡°ì–¸ì´ë‚˜ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

        3.  **ê¸°ëŠ¥ ë° ì§ˆë¬¸ ì œì•ˆ:**
            -   ë‹µë³€ í›„, ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” ê¸°ëŠ¥ì„ [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]ì—ì„œ ì°¾ì•„ ì œì•ˆí•˜ì‹­ì‹œì˜¤.
            -   ì‚¬ìš©ìê°€ ë‹¤ìŒì— ê¶ê¸ˆí•´í•  ë§Œí•œ **ê´€ë ¨ í›„ì† ì§ˆë¬¸ 3ê°€ì§€**ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ìƒì„±í•˜ì‹­ì‹œì˜¤.

        4.  **ì¶œë ¥ í˜•ì‹:** ìµœì¢… ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

        **JSON ì¶œë ¥ í˜•ì‹:**
        {{
          "answer": "ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
          "suggested_actions": ["ê°€ì¥ ê´€ë ¨ìˆëŠ” í•¨ìˆ˜ì˜ name"],
          "predicted_questions": ["ì˜ˆìƒ ì§ˆë¬¸ 1", "ì˜ˆìƒ ì§ˆë¬¸ 2", "ì˜ˆìƒ ì§ˆë¬¸ 3"]
        }}

        ---
        [ì°¸ê³  ì •ë³´]
        {context if context else "ì—†ìŒ"}
        ---
        [ì´ì „ ëŒ€í™” ë‚´ìš©]
        {history_string if history_string else "ì—†ìŒ"}
        ---
        [ì‚¬ì´íŠ¸ ê¸°ëŠ¥ ëª©ë¡]
        {functions_string}
        ---
        [í˜„ì¬ ì§ˆë¬¸]
        {query}
        ---

        JSON ì¶œë ¥:
        """
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ '{self.site_url}' ì›¹ì‚¬ì´íŠ¸ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë©°, JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"ğŸš¨ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, AI ëª¨ë¸ê³¼ í†µì‹ í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "suggested_actions": [], "predicted_questions": []}

    def ask(self, query: str, user_profile: Dict[str, Any], history: List[Dict[str, str]] = []) -> Dict[str, Any]:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        context = self._hybrid_retrieve(query)
        print(f"\n[ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸]\n---\n{context}\n---")

        response_json = self._generate_final_response(query, context, user_profile, history)

        if "suggested_actions" in response_json and isinstance(response_json["suggested_actions"], list):
            action_details = []
            valid_action_names = {func['name'] for func in self.site_functions}
            for action_name in response_json["suggested_actions"]:
                if action_name in valid_action_names:
                    for func in self.site_functions:
                        if func['name'] == action_name:
                            action_details.append({
                                "name": func['name'],
                                "description": func['description'],
                                "url": f"{self.base_url}{func['url']}"
                            })
            response_json["suggested_actions"] = action_details
        else:
            response_json["suggested_actions"] = []

        if "predicted_questions" not in response_json or not isinstance(response_json["predicted_questions"], list):
            response_json["predicted_questions"] = []

        return response_json