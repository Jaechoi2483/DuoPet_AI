"""
ì›ë³¸ ëª¨ë¸ ì‹¬ì¸µ ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê²€ì¦
Normalizationê³¼ ê°€ì¤‘ì¹˜ ë¬¸ì œì˜ ì •í™•í•œ ì›ì¸ íŒŒì•…
"""
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
import numpy as np
from pathlib import Path
import json
import h5py
import zipfile

print("ğŸ”¬ ì•ˆêµ¬ì§ˆí™˜ ëª¨ë¸ ì‹¬ì¸µ ë¶„ì„")
print("=" * 80)

# ëª¨ë¸ ê²½ë¡œë“¤
model_paths = {
    "original": Path("C:/Users/ictedu1_021/Desktop/ì•ˆêµ¬ì§ˆí™˜ëª¨ë¸/best_grouped_model.keras"),
    "converted": Path("C:/Users/ictedu1_021/Desktop/ì•ˆêµ¬ì§ˆí™˜ëª¨ë¸/final_model_fixed.keras"),
    "class_map": Path("C:/Users/ictedu1_021/Desktop/ì•ˆêµ¬ì§ˆí™˜ëª¨ë¸/class_map.json")
}

def analyze_keras_file(model_path):
    """Keras íŒŒì¼ ë‚´ë¶€ êµ¬ì¡° ë¶„ì„"""
    print(f"\nğŸ“ Keras íŒŒì¼ ë¶„ì„: {model_path.name}")
    print("-" * 60)
    
    if not model_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None
    
    # Keras íŒŒì¼ì€ ì‹¤ì œë¡œ ZIP ì•„ì¹´ì´ë¸Œ
    with zipfile.ZipFile(model_path, 'r') as zip_file:
        print("ğŸ“„ íŒŒì¼ ëª©ë¡:")
        for name in zip_file.namelist():
            info = zip_file.getinfo(name)
            print(f"  - {name}: {info.file_size:,} bytes")
        
        # config.json ì½ê¸°
        if 'config.json' in zip_file.namelist():
            with zip_file.open('config.json') as f:
                config = json.load(f)
                
            # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
            print("\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°:")
            if 'config' in config and 'layers' in config['config']:
                layers = config['config']['layers']
                print(f"ì´ ë ˆì´ì–´ ìˆ˜: {len(layers)}")
                
                # ì£¼ìš” ë ˆì´ì–´ ì°¾ê¸°
                for i, layer in enumerate(layers):
                    layer_class = layer.get('class_name', 'Unknown')
                    layer_config = layer.get('config', {})
                    layer_name = layer_config.get('name', 'unnamed')
                    
                    # ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë ˆì´ì–´ í‘œì‹œ
                    if layer_class in ['Normalization', 'BatchNormalization']:
                        print(f"\nâš ï¸ {i}: {layer_class} - {layer_name}")
                        print(f"   Config: {json.dumps(layer_config, indent=2)}")
                    
                    elif layer_class in ['Dense', 'Conv2D']:
                        units = layer_config.get('units') or layer_config.get('filters')
                        print(f"  {i}: {layer_class} - {layer_name} ({units} units/filters)")
                    
                    elif i < 3 or i >= len(layers) - 3:
                        print(f"  {i}: {layer_class} - {layer_name}")
        
        # ê°€ì¤‘ì¹˜ íŒŒì¼ í™•ì¸
        if 'model.weights.h5' in zip_file.namelist():
            print("\nğŸ“Š ê°€ì¤‘ì¹˜ íŒŒì¼ ë¶„ì„:")
            # ì„ì‹œë¡œ ì¶”ì¶œí•˜ì—¬ ë¶„ì„
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix='.h5') as tmp:
                tmp.write(zip_file.read('model.weights.h5'))
                tmp_path = tmp.name
            
            try:
                with h5py.File(tmp_path, 'r') as h5f:
                    print("ê°€ì¤‘ì¹˜ ê·¸ë£¹:")
                    
                    def print_h5_structure(name, obj):
                        if isinstance(obj, h5py.Dataset):
                            print(f"  - {name}: shape={obj.shape}, dtype={obj.dtype}")
                            # ê°€ì¤‘ì¹˜ í†µê³„
                            data = obj[()]
                            if data.size > 0:
                                print(f"    Stats: mean={np.mean(data):.4f}, std={np.std(data):.4f}, "
                                      f"min={np.min(data):.4f}, max={np.max(data):.4f}")
                                
                                # ëª¨ë“  ê°’ì´ ê°™ì€ì§€ í™•ì¸
                                if np.allclose(data, data.flat[0]):
                                    print("    âš ï¸ ëª¨ë“  ê°’ì´ ë™ì¼í•¨! (ì´ˆê¸°í™” ìƒíƒœ)")
                    
                    h5f.visititems(print_h5_structure)
            finally:
                os.unlink(tmp_path)
    
    return config

def try_load_model(model_path):
    """ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ì‹œë„ ë° ë¶„ì„"""
    print(f"\nğŸ”§ ëª¨ë¸ ë¡œë“œ ì‹œë„: {model_path.name}")
    print("-" * 60)
    
    # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë¡œë“œ ì‹œë„
    methods = [
        ("ê¸°ë³¸ ë¡œë“œ", lambda: tf.keras.models.load_model(str(model_path))),
        ("compile=False", lambda: tf.keras.models.load_model(str(model_path), compile=False)),
        ("ì»¤ìŠ¤í…€ ê°ì²´", lambda: tf.keras.models.load_model(
            str(model_path),
            custom_objects={
                'swish': tf.nn.swish,
                'Swish': tf.keras.layers.Activation(tf.nn.swish),
                'FixedDropout': tf.keras.layers.Dropout
            },
            compile=False
        ))
    ]
    
    for method_name, method_func in methods:
        print(f"\nì‹œë„: {method_name}")
        try:
            model = method_func()
            print("âœ… ë¡œë“œ ì„±ê³µ!")
            
            # ëª¨ë¸ ì •ë³´
            print(f"ì…ë ¥ shape: {model.input_shape}")
            print(f"ì¶œë ¥ shape: {model.output_shape}")
            
            # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
            test_input = np.random.random((1, 224, 224, 3)).astype(np.float32) * 255.0
            pred = model.predict(test_input, verbose=0)
            
            print(f"ì˜ˆì¸¡ ê²°ê³¼: {pred[0]}")
            print(f"í™•ë¥  ë¶„í¬: {[f'{p*100:.1f}%' for p in pred[0]]}")
            
            # ê°€ì¤‘ì¹˜ ìƒíƒœ í™•ì¸
            if np.allclose(pred[0], pred[0][0], rtol=1e-3):
                print("âŒ ëª¨ë“  ì˜ˆì¸¡ê°’ì´ ë™ì¼ - ê°€ì¤‘ì¹˜ê°€ ì´ˆê¸°í™” ìƒíƒœ!")
            else:
                print("âœ… ì˜ˆì¸¡ê°’ì´ ë‹¤ì–‘í•¨ - ê°€ì¤‘ì¹˜ ì •ìƒ")
            
            # íŠ¹ì • ë ˆì´ì–´ ê°€ì¤‘ì¹˜ í™•ì¸
            print("\nì£¼ìš” ë ˆì´ì–´ ê°€ì¤‘ì¹˜ ë¶„ì„:")
            for layer in model.layers[-5:]:  # ë§ˆì§€ë§‰ 5ê°œ ë ˆì´ì–´
                if layer.weights:
                    print(f"\n{layer.name}:")
                    for weight in layer.weights:
                        w_data = weight.numpy()
                        print(f"  - {weight.name}: shape={w_data.shape}")
                        print(f"    mean={np.mean(w_data):.4f}, std={np.std(w_data):.4f}")
            
            return model
            
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {e}")
            
            # Normalization ë¬¸ì œì¸ì§€ í™•ì¸
            if "normalization" in str(e).lower():
                print("â†’ Normalization ë ˆì´ì–´ ë¬¸ì œ í™•ì¸ë¨")
                print(f"ìƒì„¸ ì˜¤ë¥˜: {type(e).__name__}")
    
    return None

def analyze_weight_initialization():
    """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ìƒíƒœ ë¶„ì„"""
    print("\nğŸ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” íŒ¨í„´ ë¶„ì„")
    print("-" * 60)
    
    # Dense ë ˆì´ì–´ì˜ ì¼ë°˜ì ì¸ ì´ˆê¸°í™” íŒ¨í„´
    print("Dense ë ˆì´ì–´ ì´ˆê¸°í™” íŒ¨í„´ (Glorot uniform):")
    
    # 5ê°œ ì¶œë ¥ì„ ê°€ì§„ Dense ë ˆì´ì–´ì˜ ì´ˆê¸°í™” ì‹œë®¬ë ˆì´ì…˜
    np.random.seed(42)
    fan_in, fan_out = 128, 5  # ì˜ˆ: 128ê°œ ì…ë ¥, 5ê°œ ì¶œë ¥
    limit = np.sqrt(6.0 / (fan_in + fan_out))
    weights = np.random.uniform(-limit, limit, size=(fan_in, fan_out))
    
    # Softmax ì ìš©
    logits = np.random.randn(1, 5)  # ì„ì˜ì˜ ì…ë ¥
    output = np.dot(np.ones((1, fan_in)) * 0.1, weights)  # ì‘ì€ ì…ë ¥ê°’
    softmax_output = np.exp(output) / np.sum(np.exp(output))
    
    print(f"ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ ë²”ìœ„: [{-limit:.4f}, {limit:.4f}]")
    print(f"Softmax ì¶œë ¥ (ì´ˆê¸°í™” ìƒíƒœ): {softmax_output[0]}")
    print(f"ê° í´ë˜ìŠ¤ í™•ë¥ : {[f'{p*100:.1f}%' for p in softmax_output[0]]}")
    
    if np.allclose(softmax_output[0], 0.2, atol=0.05):
        print("â†’ ì´ˆê¸°í™” ìƒíƒœì—ì„œëŠ” ëŒ€ëµ 20%ì”© ê· ë“± ë¶„í¬ê°€ ì •ìƒ")

# ì‹¤í–‰
print("\n" + "="*80)
print("ğŸš€ ë¶„ì„ ì‹œì‘\n")

# 1. ì›ë³¸ ëª¨ë¸ ë¶„ì„
if model_paths["original"].exists():
    print("1ï¸âƒ£ ì›ë³¸ ëª¨ë¸ ë¶„ì„")
    config = analyze_keras_file(model_paths["original"])
    model = try_load_model(model_paths["original"])

# 2. ë³€í™˜ëœ ëª¨ë¸ ë¶„ì„
if model_paths["converted"].exists():
    print("\n\n2ï¸âƒ£ ë³€í™˜ëœ ëª¨ë¸ ë¶„ì„")
    config = analyze_keras_file(model_paths["converted"])
    model = try_load_model(model_paths["converted"])

# 3. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¶„ì„
analyze_weight_initialization()

# 4. ê²°ë¡ 
print("\n\nğŸ“‹ ë¶„ì„ ê²°ë¡ :")
print("="*80)
print("1. Normalization ë ˆì´ì–´ê°€ adapt() ì—†ì´ ì €ì¥ë˜ì–´ mean/varianceê°€ ì—†ìŒ")
print("2. ëª¨ë“  ì˜ˆì¸¡ì´ 20%ì¸ ê²ƒì€ ê°€ì¤‘ì¹˜ê°€ ì´ˆê¸°í™” ìƒíƒœì„ì„ ì˜ë¯¸")
print("3. ëª¨ë¸ì´ ì‹¤ì œë¡œ í•™ìŠµë˜ì§€ ì•Šì•˜ê±°ë‚˜ ê°€ì¤‘ì¹˜ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì§€ ì•ŠìŒ")
print("\nğŸ’¡ í•´ê²° ë°©ì•ˆ:")
print("- Normalizationì„ ì œê±°í•˜ê³  Lambda ë ˆì´ì–´ë¡œ ëŒ€ì²´")
print("- ê°€ì¤‘ì¹˜ë¥¼ ë³„ë„ë¡œ ì €ì¥/ë¡œë“œí•˜ê±°ë‚˜")
print("- ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµ")