"""
í”¼ë¶€ ì§ˆí™˜ ëª¨ë¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ êµ¬ì¡°ì™€ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""
import os
import tensorflow as tf
import numpy as np
from pathlib import Path
import json

def analyze_skin_models():
    """í”¼ë¶€ ì§ˆí™˜ ëª¨ë¸ë“¤ì„ ë¶„ì„"""
    
    base_dir = Path("models/health_diagnosis/skin_disease")
    print("ğŸ” í”¼ë¶€ ì§ˆí™˜ ëª¨ë¸ ë¶„ì„ ì‹œì‘...\n")
    
    # ë¶„ë¥˜ ëª¨ë¸ ê²½ë¡œë“¤
    classification_models = {
        "dog_binary": base_dir / "classification/dog_binary/dog_binary_model.h5",
        "cat_binary": base_dir / "classification/cat_binary/cat_binary_model.h5",
        "dog_multi_136": base_dir / "classification/dog_multi_136/dog_multi_136_model.h5",
        "dog_multi_456": base_dir / "classification/dog_multi_456/dog_multi_456_model.h5"
    }
    
    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ê²½ë¡œë“¤
    segmentation_models = {
        "dog_A1": base_dir / "segmentation/dog_A1",
        "dog_A2": base_dir / "segmentation/dog_A2",
        "dog_A3": base_dir / "segmentation/dog_A3",
        "dog_A4": base_dir / "segmentation/dog_A4",
        "dog_A5": base_dir / "segmentation/dog_A5",
        "dog_A6": base_dir / "segmentation/dog_A6",
        "cat_A2": base_dir / "segmentation/cat_A2"
    }
    
    print("=" * 60)
    print("ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ë¶„ì„")
    print("=" * 60)
    
    analysis_results = {"classification": {}, "segmentation": {}}
    
    for name, model_path in classification_models.items():
        print(f"\nğŸ”¹ {name} ëª¨ë¸ ë¶„ì„")
        print(f"   ê²½ë¡œ: {model_path}")
        
        if not model_path.exists():
            print(f"   âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            continue
            
        try:
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = model_path.stat().st_size / (1024 * 1024)  # MB
            print(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„
            print(f"   ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
            model = tf.keras.models.load_model(str(model_path), compile=False)
            print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            
            # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
            print(f"\n   ğŸ“ ëª¨ë¸ êµ¬ì¡°:")
            print(f"   - ì…ë ¥ shape: {model.input_shape}")
            print(f"   - ì¶œë ¥ shape: {model.output_shape}")
            print(f"   - ì´ ë ˆì´ì–´ ìˆ˜: {len(model.layers)}")
            print(f"   - ì´ íŒŒë¼ë¯¸í„°: {model.count_params():,}")
            
            # ë ˆì´ì–´ íƒ€ì… ë¶„ì„
            layer_types = {}
            for layer in model.layers:
                layer_type = type(layer).__name__
                layer_types[layer_type] = layer_types.get(layer_type, 0) + 1
            
            print(f"\n   ğŸ”§ ë ˆì´ì–´ êµ¬ì„±:")
            for layer_type, count in layer_types.items():
                print(f"   - {layer_type}: {count}ê°œ")
            
            # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
            print(f"\n   ğŸ§ª í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡:")
            test_input = np.random.rand(1, 224, 224, 3).astype(np.float32)
            
            try:
                # TF 1.x ìŠ¤íƒ€ì¼ ì˜ˆì¸¡
                predictions = model.predict(test_input, verbose=0)
                print(f"   âœ… ì˜ˆì¸¡ ì„±ê³µ!")
                print(f"   - ì¶œë ¥ shape: {predictions.shape}")
                print(f"   - ì¶œë ¥ ë²”ìœ„: [{predictions.min():.4f}, {predictions.max():.4f}]")
                print(f"   - ì¶œë ¥ í•©ê³„: {predictions.sum():.4f}")
                
                # í™œì„±í™” í•¨ìˆ˜ ì¶”ì •
                if abs(predictions.sum() - 1.0) < 0.01:
                    print(f"   - ì¶”ì • í™œì„±í™”: softmax (í•©ê³„ â‰ˆ 1)")
                elif predictions.max() <= 1.0 and predictions.min() >= 0.0:
                    print(f"   - ì¶”ì • í™œì„±í™”: sigmoid (0-1 ë²”ìœ„)")
                else:
                    print(f"   - ì¶”ì • í™œì„±í™”: ê¸°íƒ€")
                    
            except Exception as e:
                print(f"   âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ ì €ì¥
            analysis_results["classification"][name] = {
                "exists": True,
                "file_size_mb": file_size,
                "input_shape": str(model.input_shape),
                "output_shape": str(model.output_shape),
                "total_params": model.count_params(),
                "loadable": True,
                "predictable": predictions is not None if 'predictions' in locals() else False
            }
            
        except Exception as e:
            print(f"   âŒ ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis_results["classification"][name] = {
                "exists": True,
                "loadable": False,
                "error": str(e)
            }
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¶„ì„")
    print("=" * 60)
    
    for name, model_path in segmentation_models.items():
        print(f"\nğŸ”¹ {name} ëª¨ë¸ ë¶„ì„")
        print(f"   ê²½ë¡œ: {model_path}")
        
        if not model_path.exists():
            print(f"   âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            analysis_results["segmentation"][name] = {"exists": False}
            continue
            
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸ - ì—¬ëŸ¬ íŒ¨í„´ ê²€ìƒ‰
        data_files = list(model_path.glob("*.data-*"))
        index_files = list(model_path.glob("*.index"))
        checkpoint_files = list(model_path.glob("checkpoint"))
        
        all_checkpoint_files = data_files + index_files + checkpoint_files
        
        if all_checkpoint_files:
            print(f"   âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ë°œê²¬: {len(all_checkpoint_files)}ê°œ")
            # íŒŒì¼ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜í•´ì„œ í‘œì‹œ
            if data_files:
                print(f"      ğŸ“Š ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ")
                for f in data_files[:2]:
                    print(f"         - {f.name}")
            if index_files:
                print(f"      ğŸ“‘ ì¸ë±ìŠ¤ íŒŒì¼: {len(index_files)}ê°œ")
                for f in index_files[:2]:
                    print(f"         - {f.name}")
            if checkpoint_files:
                print(f"      ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼: {len(checkpoint_files)}ê°œ")
                for f in checkpoint_files:
                    print(f"         - {f.name}")
        else:
            print(f"   âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
        analysis_results["segmentation"][name] = {
            "exists": True,
            "checkpoint_count": len(all_checkpoint_files),
            "data_files": len(data_files),
            "index_files": len(index_files),
            "checkpoint_files": len(checkpoint_files)
        }
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    result_path = Path("skin_models_analysis.json")
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n\n{'='*60}")
    print("ğŸ“Š ë¶„ì„ ìš”ì•½")
    print("="*60)
    
    # ë¶„ë¥˜ ëª¨ë¸ ìš”ì•½
    total_classification = len(classification_models)
    loadable_classification = sum(1 for r in analysis_results["classification"].values() 
                                if r.get("loadable", False))
    
    print(f"\në¶„ë¥˜ ëª¨ë¸:")
    print(f"  - ì „ì²´: {total_classification}ê°œ")
    print(f"  - ë¡œë“œ ê°€ëŠ¥: {loadable_classification}ê°œ")
    
    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ìš”ì•½
    total_segmentation = len(segmentation_models)
    existing_segmentation = sum(1 for r in analysis_results["segmentation"].values() 
                              if r.get("exists", False))
    
    print(f"\nì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸:")
    print(f"  - ì „ì²´: {total_segmentation}ê°œ")
    print(f"  - ì¡´ì¬í•¨: {existing_segmentation}ê°œ")
    
    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ {result_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return analysis_results

if __name__ == "__main__":
    results = analyze_skin_models()
    
    # ë³€í™˜ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
    print("\n\nğŸ¯ ë³€í™˜ ê°€ëŠ¥ì„± í‰ê°€:")
    
    can_convert = False
    for name, info in results["classification"].items():
        if info.get("loadable", False):
            print(f"  âœ… {name}: TF 2.xë¡œ ë³€í™˜ ê°€ëŠ¥")
            can_convert = True
        else:
            print(f"  âŒ {name}: ë³€í™˜ ë¶ˆê°€ëŠ¥")
    
    if can_convert:
        print("\nâœ¨ ì¼ë¶€ ëª¨ë¸ì€ TF 2.xë¡œ ë³€í™˜ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        print("ğŸ’¡ fix_skin_model_tf2_correct.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ë³€í™˜ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")